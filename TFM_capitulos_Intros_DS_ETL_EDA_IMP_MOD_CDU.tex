\documentclass{book}
\renewcommand{\tablename}{Tabla}
\usepackage[latin1]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[font=small,labelfont=bf]{caption}
% Bibliografía
\usepackage[backend=biber]{biblatex}
\addbibresource{TFMbib.bib}
% Bookmarks, hidelinks(hyperlinks sin caja roja), linktocpage(link desde el número, no el nombre)
\usepackage[bookmarks,hidelinks,linktocpage=true]{hyperref}
% Quitar encabezado de las páginas en blanco
\usepackage{emptypage}

\title{TFM}

\begin{document}
\emergencystretch 3em % Evita que los nombres largos de los ficheros ocupen los márgenes 

\begin{titlepage} % Suppresses headers and footers on the title page

	\centering % Centre everything on the title page
	
  \begin{figure}
	\centering
	\includegraphics[scale=0.4]{UAH-logo}
	\end{figure}
	
	
	\scshape % Use small caps for all text on the title page

	\vfill
	\Large Máster en Data Science
	\vfill
	
	\LARGE{Trabajo Fin de Máster}

	
	\vspace*{\baselineskip} % White space at the top of the page
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{3.2pt} % Thick horizontal rule
	\rule{\textwidth}{0.4pt} % Thin horizontal rule
	
	\vspace{0.75\baselineskip} % Whitespace above the title
	
	{\LARGE PREDICCIÓN DE OCUPACIÓN DE PARQUÍMETROS\\ SEGÚN MODELOS PREDICTIVOS ESPACIO-TEMPORALES\\} % Title
	
	\vspace{0.75\baselineskip} % Whitespace below the title
	
	\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal rule
	\rule{\textwidth}{1.6pt} % Thick horizontal rule
	
	\vspace{2\baselineskip} % Whitespace after the title block
	
	%------------------------------------------------
	%	Subtitle
	%------------------------------------------------
	
	
	\vspace*{3\baselineskip} % Whitespace under the subtitle
	
	%------------------------------------------------
	%	Editor(s)
	%------------------------------------------------
	
	Alumnos:
	
	\vspace{0.5\baselineskip} % Whitespace before the editors
	
	{\scshape\Large Eva Carbón \\ Emilio Delgado \\ Cintia García \\ Paloma Panadero\\ Pedro Sánchez \\} % Editor list
	
	\vspace{0.5\baselineskip} % Whitespace below the editor list
	
 % Editor affiliation
	

\end{titlepage}

\tableofcontents

\listoffigures

\listoftables


\chapter{Introducción}
\section{Objetivo}
En este documento de trabajo fin de máster analizamos en profundidad el uso de técnicas de Big Data y Aprendizaje Automático para la predicción de porcentajes de ocupación de las zonas de aparcamiento reguladas por parquímetros de la ciudad de Seattle.
\smallbreak
El resultado de este trabajo se puede aprovechar para la creación o mejora de aplicaciones móviles (apps) asociadas al uso de aparcamientos regulados por parquímetros. 
\smallbreak
Son muchas las apps para aparcar disponibles para smartphones pero el propio mercado de oferta y demanda ha eliminado competencia, algunas han ido desapareciendo, y otras tienen un ámbito de actuación restringido (sólo en algunos municipios y ciudades). Una de las funcionalidades más demandadas por los usuarios y por los operadores de aparcamiento a los desarrolladores de las apps es que puedan dar información de la situación de ocupación.
\smallbreak
En el caso de la ciudad de Barcelona, la consultora \textit{AIS Group} ha logrado desarrollar una app para informar a los conductores sobre las plazas disponibles en el momento de la conducción, sea ese momento presente o futuro, en modo predictivo. También como en nuestro caso, los aparcamientos objeto de predicción son aquellos de estacionamiento regulado \cite{Objone}.
\smallbreak
\textit{Find \& Pay} es el nombre de otra app que se encuentra ahora mismo en fase de prueba y testeo. Es la mayor app de aparcamiento en Europa, con presencia en casi 600 ciudades de once países distintos, con más de 500 probadores en 31 ciudades europeas que testean, validan y mejoran su capacidad predictiva \cite{Objtwo}. Esta app, desarrollada por \textit{EasyPark}, utiliza algoritmos avanzados para procesar diversas fuentes de datos, incluidos datos de transacciones, datos de seguimiento de dispositivos, datos de sensores y datos de automóviles en circulación, entre otros \cite{Objthree}.
\smallbreak
También hemos encontrado que la app \textit{OPnGO} utiliza modelos predictivos para ayudar a los conductores a encontrar plaza en las zonas de estacionamiento regulado en distintas ciudades de Francia, España, Bélgica, Luxemburgo y Brasil \cite{Objfour}.
\smallbreak
Por último, mencionar la app \textit{Telpark} que permite hasta el pago de denuncias, como servicio adicional a los mencionados anteriormente. Sus servicios están ya consolidados en decenas de ciudades españolas y también utiliza los modelos predictivos para su funcionamiento \cite{Objfive}.

\section{Background del problema}
Creemos en la bondad de este estudio y de su desarrollo futuro para ayudar a la población en general, debido a todos los \textbf{beneficios} que puede aportar el hecho de anticipar el conocimiento de las plazas libres en una determinada zona.
\smallbreak
Uno de los beneficios más evidentes es el \underline{ahorro de tiempo} para el propio conductor. La población pierde numerosos minutos de su vida buscando aparcamiento, dando vueltas a la misma manzana esperando que se libere una plaza. Ésto repercute negativamente en la vida de las personas, ya que deben prever un tiempo suplementario que perderán en buscar aparcamiento para poder llegar a la hora a su cita. Y llegar puntual sería el mejor de los casos, ya que los retrasos en las citas son frecuentes debido a las dificultades para aparcar. Algunas cifras a modo de ejemplo:
\begin{itemize}
\item un 30\% del volumen del tráfico del centro de las ciudades es causado por coches buscando aparcamiento
\item en media un usuario pierde 20 minutos cada vez que busca aparcamiento
\item el 32\% de las multas que se extienden en Madrid son por estacionamiento incorrecto \cite{Backone}
\item en la ciudad de Londres un conductor pierde de media 67 horas al año buscando aparcamiento \cite{Backtwo} \item en EEUU la media es de 17 horas perdidas, lo que resulta en un montante económico de 345\$ por persona, teniendo en cuenta el coste de las emisiones, gasolina y tiempo
\item y concretamente en la ciudad de Seattle se pierden 58 horas al año en esta búsqueda, lo que monetariamente se traduce en 1.205\$ por persona perdidos al año \cite{Backthree}
\end{itemize}
Otra ventaja asociada al uso de un predictor de ocupación en zonas reguladas por parquímetros es la \underline{reducción de contaminación}. Una persona que no tiene a su disposición esta información daría vueltas por la zona deseada hasta encontrar aparcamiento, dando lugar a un gasto extra de gasolina y también un alto nivel de contaminación asociado, ya que precisamente cuando buscamos plazas libres conducimos en marchas cortas, que son las que más efectos contaminantes tienen. Por ilustrar este dato, en la ciudad alemana de Freiburg el 74\% del tráfico de la ciudad se debe a conductores buscando aparcamiento \cite{Backfour}. En Los Ángeles este nivel de tráfico llega al 30\% \cite{Backfive}. Las emisiones de gases de efecto invernadero se verían reducidas considerablemente si se consigue reducir el tiempo de búsqueda de aparcamiento. Los expertos en movilidad tienen incluso un nombre para este fenómeno: tráfico de agitación.
\smallbreak
También altera el humor de las personas, la espera en general hace que nos pongamos más nerviosos y perjudica nuestra actividad cardíaca. La \underline{felicidad} de la población se ve afectada por esta espera en la búsqueda de aparcamiento, generando además peleas entre conductores que se disputan una misma plaza. En este sentido, predecir la ocupación en una determinada zona hará que el conductor sepa si por ejemplo tiene que irse a otra zona colindante para aumentar sus posibilidades de aparcar más rápido, haciendo que no tenga que perder la paciencia en la zona con nivel de ocupación más alto. De media dos tercios de las personas que se ven obligadas a buscar aparcamiento confiesan sentirse estresadas en esos momentos \cite{Backsix}. 
\smallbreak
Y no hay que olvidar el beneficio \underline{comercial}, pues el hecho de que una zona suela tener problemas para poder aparcar ahuyenta a posibles compradores de acudir a esa zona a visitar los comercios locales. Así, si se sabe con antelación la ocupación de una determinada área, será más fácil animar al consumidor a acudir a los comercios en ese área.
\smallbreak
El \underline{transporte público} existente en la ciudad también se vería beneficiado de la puesta a disposición del público de las predicciones sobre ocupación que vamos a exponer, ya que en el caso de que una zona urbana esté masivamente ocupada, el usuario podría tender a dejar aparcado el coche en casa y optar por los servicios públicos de transporte para llegar a su punto de destino.



\chapter{Fuentes de Datos}
En este capítulo presentamos las múltiples fuentes de datos que hemos considerado para el análisis.
\smallbreak
Hemos comenzado buscando en Internet datasets públicos con datos de uso de parquímetros, concretamente sus tickets o transacciones. Aunque como preveíamos la disponibilidad pública de este tipo de información es muy escasa, el dataset elegido como fuente de datos para el TFM no ha sido el único que hemos encontrado. Hemos descartado por comparación en número de registros el uso de un dataset de la ciudad de Melbourne, porque su tamaño es mucho menor (más de 342 mil registros) y por estar limitado temporalmente a un único año \cite{FDone}. Y también hemos descartado el uso de otro dataset con más registros porque la información de ocupación corresponde a la utilización de aparcamientos privados en la ciudad de Bath, y nuestro objetivo era obtener datos del uso de aparcamientos públicos, es decir, de parquímetros en la calle \cite{FDtwo}.
\smallbreak
El dataset con transacciones de uso de parquímetros públicos que hemos seleccionado como fuente principal de datos de nuestro TFM lo hemos encontrado en un Github con la documentación publicada por Rex Thompson como proyecto final de sus estudios de Data Science en la Universidad de Washington en 2017, y cuyo objetivo de estudio es totalmente diferente del nuestro. El objetivo de Rex Thompson era el análisis de los registros de los parquímetros para calcular el dinero total recaudado por la ciudad de Seattle durante las horas en las que hay fijadas restricciones de aparcamiento \cite{FDthree}.
\smallbreak
Los datos en crudo originales recopilados en el Github mencionado pertenecen al departamento de transportes de Seattle, conocido como SDOT (the city of Seattle Deparment of Transportation), que indica en su web que pone a disposición pública esta información con el objetivo de animar a los desarrolladores a crear aplicaciones que puedan ayudar a los usuarios a encontrar aparcamiento más rápidamente y pasar menos tiempo circulando o en atascos.
\smallbreak
SDOT indica que los parquímetros de Seattle operan de Lunes a Sábado entre las 8am y las 8pm, con límites de tiempo de uso que pueden variar entre las 2, 4 o 10 horas. Y en periodos de desplazamientos al trabajo por la mañana y por la tarde-noche no está permitido el aparcamiento en algunas calles principales del centro de la ciudad. Añade también que el cálculo de la ocupación (número de transacciones dividido por el número de plazas disponibles en un periodo) no reflejaría la situación real ya que hay vehículos que no pagan (por causas justificadas o no).
\newpage
En el proyecto de Rex Thompson se utilizan dos datasets que publica SDOT mediante APIs:
\begin{itemize}
\item \textit{Paid Parking information data} \cite{FDfour}, que contiene un histórico de transacciones desde Enero de 2012 a Septiembre de 2017, y en el que cada registro contiene como variables de interés para nuestro proyecto las siguientes:
	\begin{itemize}
	\item \textit{TransactionId}: identificador único de la transacción realizada en el parquímetro
	\item \textit{TransactionDateTime}: fecha y hora de la transacción
	\item \textit{Duration\_mins}: duración en minutos reservada para el aparcamiento
	\item \textit{ElementKey}: identificador del segmento de la calle donde se ubica el parquímetro
	\end{itemize}
\item \textit{Parking Blockface information data} \cite{FDfive}, que descarga un fichero pequeño llamado \textit{'Blockface.csv'} que complementa al dataset anterior y contiene como variables de interés para nuestro proyecto las siguientes:
	\begin{itemize}
	\item \textit{ElementKey}: coincide con el dataset anterior
	\item \textit{ParkingSpaces}: el número de plazas de parking disponibles en el segmento de calle
	\item \textit{PaidParkingArea}: el barrio o distrito de la ciudad al que está asociado el segmento de calle
	\end{itemize}
\end{itemize}

Hemos aprovechado el trabajo laborioso ya realizado y compartido por Rex Thompson de recogida, limpieza y consolidación de los datos del primer dataset de transacciones, ya que SDOT sólo permite consultas que obtienen como respuesta ficheros con información de un máximo de 7 días. El fichero global de transacciones consolidado por Rex Thompson para el periodo entre el 1 de Enero de 2012 y el 30 de Septiembre de 2017 tiene un tamaño de 5.32GB y más de 62 millones de registros, y se llama \textit{'ParkingTransaction\_20120101\_20170930\_cleaned.csv'} (\textbf{DATASET-1}). También aprovechamos la limpieza realizada sobre el segundo dataset y utilizamos el fichero disponible llamado \textit{'Blockface\_cleaned.csv'} (\textbf{DATASET-2}).

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{FD-dataset1}
		\caption{Extracto de las primeras muestras del DATASET-1}
		\label{FD-dataset1}
	\end{figure}
	
\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{FD-dataset2}
		\caption{Extracto de las primeras muestras del DATASET-2}
		\label{FD-dataset2}
	\end{figure}
	
Para nuestro proyecto necesitábamos añadir a los dos datasets mencionados los datos geoespaciales de localización de los parquímetros. Por ello hemos buscado en Internet las localizaciones GPS con latitud y longitud asociadas a los parquímetros de la ciudad de Seattle y hemos encontrado que SDOT publica también esa información a través de una API \cite{FDsix} \cite{FDsixb}. Hemos creado un notebook de Python llamado \textit{'FD\_SDOT\_PayStations.ipynb'} para realizar las consultas a esa API y descargar la información en dos ficheros json (\textit{'paystations\_ids\_1\_1000.json'} y \textit{'paystations\_ids\_1001\_1800.json'}). Son necesarios dos ficheros debido a que la API fija un límite de respuesta de 1000 registros por consulta. En la segunda parte del mismo notebook seleccionamos los tres parámetros que nos interesan de los ficheros json:
	\begin{itemize}
	\item \textit{ELMNTKEY}: identificador del segmento de calle que coincide con los 2 datasets de partida
	\item \textit{SHAPE\_LAT}: latitud de coordenadas GPS
	\item \textit{SHAPE\_LNG}: longitud de coordenadas GPS
	\end{itemize}

Luego hemos unido los datos en un dataframe de Pandas calculando la media de las distintas coordenadas existentes para un mismo \textit{element key} antes de escribirlo en un fichero csv llamado \textit{'Coord\_EK.csv'} que reutilizaremos como dataset en otros notebooks (\textbf{DATASET-3}). Como habíamos indicado anteriormente el identificador \textit{element key} hace referencia a un segmento de calle, y dependiendo de la longitud del segmento podemos tener hasta 3 coordenadas distintas para un mismo \textit{element key}, por eso calculando la media de los valores de coordenadas existentes para un mismo \textit{element key} obtenemos las coordenadas asociadas al punto central del segmento de calle asociado al \textit{element key}. Hemos asumido por simplicidad que un element key identifica a un único parquímetro.

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset3}
		\caption{Extracto de las primeras muestras del DATASET-3}
		\label{FD-dataset3}
	\end{figure}

Asociado también a la ciudad de Seattle hemos encontrado en Kaggle \cite{FDsixc} un dataset que contiene información meteorológica histórica desde 1948 hasta 2017 (\textbf{DATASET-4}: fichero \textit{'seattleWeather\_1948-2017.csv'}) \cite{FDseven}. Cada registro de este dataset meteorológico contiene las siguientes variables de interés para nuestro proyecto: 
	\begin{itemize}
	\item \textit{DATE}: fecha de observación
	\item \textit{PRCP}: cantidad de precipitación medida en pulgadas
	\item \textit{TMAX}: temperatura máxima del día medida en grados Farenheit
	\item \textit{TMIN}: temperatura mínima del día medida en grados Farenheit
	\end{itemize}
	
	\newpage
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset4}
		\caption{Extracto de las primeras muestras del DATASET-4}
		\label{FD-dataset4}
	\end{figure}
	
También relacionado con la meteorología hemos encontrado un dataset con registros asociados a sensores de temperatura ubicados en la ciudad de Seattle que recogen datos de temperatura ambiente y del asfalto por minuto desde Marzo de 2014 hasta hoy (\textbf{DATASET-5}: fichero \textit{'Road\_Weather\_Information\_Stations.csv'} \cite{FDeight}). 

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset5}
		\caption{Extracto de las primeras muestras del DATASET-5}
		\label{FD-dataset5}
	\end{figure}
	
A diferencia del dataset anterior en el que los datos son diarios, en este dataset se dispone de datos medidos cada minuto y recogidos en 10 estaciones con distintas ubicaciones. Hemos creado un notebook de Python llamado \textit{'FD\_Road\_Weather\_Information\_Stations.ipynb'} que transforma el dataset original para poder combinarlo con el dataset de transacciones. Entre las transformaciones necesarias destacamos las siguientes:
	\begin{itemize}
	\item filtrado de las horas en el rango de horas hábiles de uso de los parquímetros y agregación por horas de las medidas por minuto
	\item conversión de medidas de grados Farenheit a Celsius
	\item cálculo de la estación meteorológica de medida más próxima a cada parquímetro utilizando la distancia Haversine que es la que se usa habitualmente para calcular distancias entre puntos ubicados con coordenadas GPS ya que tiene en cuenta la curvatura de la tierra. Generamos el fichero \textit{'Coord\_EK\_stations.csv'} (\textbf{DATASET-6})
	\item completado de la serie temporal realizando interpolación porque faltan datos para algunos días y horas que provocarían nulos indeseados en la combinación con el dataset de transacciones, creando el fichero \textit{'RWIS\_completed.csv'} (\textbf{DATASET-7})
	\end{itemize}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset6}
		\caption{Extracto de las primeras muestras del DATASET-6}
		\label{FD-dataset6}
	\end{figure}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset7}
		\caption{Extracto de las primeras muestras del DATASET-7}
		\label{FD-dataset7}
	\end{figure}

\newpage
En relación con las ubicaciones de los parquímetros en la ciudad de Seattle hemos buscado información sobre su proximidad a puntos de interés cultural o deportivo en la ciudad, ya que su ocupación puede estar condicionada por esa situación. En la web de datos públicos de la ciudad de Seattle hemos encontrado ambas informaciones. Por un lado con un dataset que ubica teatros, cines, museos, bibliotecas, galerías, clubs de música, etc \cite{FDnine} (\textbf{DATASET-8}: fichero \textit{'Seattle\_Cultural\_Space\_Inventory.csv'}). 

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{FD-dataset8}
		\caption{Extracto de las primeras muestras del DATASET-8}
		\label{FD-dataset8}
	\end{figure}

\newpage
Y por otro lado con varios datasets que ubican instalaciones deportivas en la ciudad para practicar diferentes deportes \cite{FDten}:
	\begin{itemize}
	\item baseball: fichero \textit{'Baseball\_Field.csv'} (\textbf{DATASET-9})
	\item tenis: fichero \textit{'Tennis\_Court\_Point.csv'} (\textbf{DATASET-10})
	\item natación: fichero \textit{'Swimming\_Pools.csv'} (\textbf{DATASET-11}) 
	\item baloncesto: fichero \textit{'Basketball\_Court\_Point.csv'} (\textbf{DATASET-12})
	\item futbol: fichero \textit{'Soccer\_Field.csv'} (\textbf{DATASET-13})
	\item atletismo: fichero \textit{'Track\_Fields.csv'} (\textbf{DATASET-14})
	\end{itemize}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{FD-dataset9}
		\caption{Extracto de las primeras muestras del DATASET-9}
		\label{FD-dataset9}
	\end{figure}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{FD-dataset10}
		\caption{Extracto de las primeras muestras del DATASET-10}
		\label{FD-dataset10}
	\end{figure}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{FD-dataset11}
		\caption{Extracto de las primeras muestras del DATASET-11}
		\label{FD-dataset11}
	\end{figure}

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{FD-dataset12}
		\caption{Extracto de las primeras muestras del DATASET-12}
		\label{FD-dataset12}
	\end{figure}
	
\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{FD-dataset13}
		\caption{Extracto de las primeras muestras del DATASET-13}
		\label{FD-dataset13}
	\end{figure}	

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{FD-dataset14}
		\caption{Extracto de las primeras muestras del DATASET-14}
		\label{FD-dataset14}
	\end{figure}

\newpage
Hemos creado un notebook de Python llamado \textit{'FD\_Cultural\_And\_Sports\_Points.ipynb'} para combinar estos 7 datasets con el \textbf{DATASET-3} y crear un nuevo dataset a partir de éste último. Con el notebook descargamos los distintos ficheros csv a partir de APIs, seleccionamos las variables de interés de cada dataset, realizamos una pequeña limpieza y combinamos los registros con el \textbf{DATASET-3} para calcular la distancia Haversine entre los parquímetros y los puntos de interés (culturales y deportivos). En el nuevo dataset contenido en el fichero \textit{'Coord\_cult\_\&\_sport.csv'} (\textbf{DATASET-15}) hemos creado una nueva columna binaria por cada tipo de punto de interés en el que señalamos con un valor 1 aquellos parquímetros que tienen un punto de interés a una distancia inferior a 75 metros, y con un valor 0 al resto. Observamos que con esa distancia se descartan los datasets asociados a natación y atletismo porque no hay ningún parquímetro cerca.

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset15}
		\caption{Extracto de las primeras muestras del DATASET-15}
		\label{FD-dataset15}
	\end{figure}

Además también hemos buscado información sobre eventos de interés en la ciudad que pudieran influir en el uso de los parquímetros, y en la web de datos públicos de la ciudad de Seattle hemos encontrado un dataset con algunos eventos para varios meses del año 2016 \cite{FDeleven} (fichero \textit{'City\_of\_Seattle\_Events.csv'}). Hemos creado un notebook de Python llamado \textit{'FD\_Eventos\_Seattle\_2016.ipynb'} para descargar el fichero a través de API y combinar esa información con un dataframe manual que hemos creado con otros eventos que hemos encontrado en internet de forma independiente. Este dataset (\textbf{DATASET-16}: fichero \textit{'Events\_2016.csv'}) se combinará con el dataset de transacciones para calcular la distancia Haversine e identificar por parquímetro y fecha las transacciones con un evento cerca y en ese día concreto.

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset16}
		\caption{Extracto de las primeras muestras del DATASET-16}
		\label{FD-dataset16}
	\end{figure}

Y por último hemos encontrado en internet un formulario para consultar información sobre la calidad del aire medida en las principales ciudades de Estados Unidos \cite{FDtwelve}. Hemos seleccionado la ciudad de Seattle, el año 2016 y los cuatro parámetros siguientes que son los considerados más relevantes para medir la polución del aire y que se relacionan con el motor y tubo de escape de los vehículos:
\newpage
	\begin{itemize}
	\item monóxido de carbono (CO)
	\item dióxido de nitrógeno ($NO_2$) 
	\item ozono ($O_3$)
	\item partículas en suspensión de 2,5 micrómetros o menos ($PM_{2.5}$)
	\end{itemize}

Hemos combinado el resultado de las cuatro consultas en un dataset mediante un sencillo notebook llamado \textit{'FD\_Air\_Quality\_Data\_Seattle\_conversion.ipynb'} en el que además hemos unificado la unidad de medida de las tres primeras variables como $\mu$g/$m^3$ a partir de fórmulas encontradas en internet \cite{FDthirteen} (\textbf{DATASET-17}: fichero \textit{'Air\_Quality\_Data\_Seattle\_2016.csv'}).

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{FD-dataset17}
		\caption{Extracto de las primeras muestras del DATASET-17}
		\label{FD-dataset17}
	\end{figure}

En resumen, hemos recopilado diversas fuentes de datos que podemos dividir en dos grupos. Los tres primeros datasets son los necesarios para construir una serie espacio-temporal del porcentaje de ocupación de plazas de parking. Y los datasets restantes son complementarios para añadir a esa serie variables adicionales que pueden tener influencia en el porcentaje de ocupación mencionado y por tanto ayudar a su predicción futura.



\chapter{ETL de los Datos}
Hemos creado un notebook de Python llamado \textit{'ETL\_Seattle\_serie\_2016.ipynb'} para realizar las tareas de preprocesamiento de las fuentes de datos que podríamos asimilar a los procesos de Extracción, Transformación y Carga, en los que se extraen datos desde múltiples fuentes, se limpian, manipulan o reformatean para luego cargarlos en este caso en otro fichero final que es el que se utilizará para crear los modelos de predicción.
\smallbreak
Decidimos acotar el análisis de las fuentes de datos al año 2016 porque es el año más reciente para el que tenemos datos todos los meses en el dataset inicial de transacciones (\textbf{DATASET-1}) y para simplificar el tamaño del dataset ya que sólo con ese año tiene casi 11 millones de registros.
\smallbreak
En el notebook mencionado realizamos las siguientes acciones destacables sobre el \textbf{DATASET-1} (transacciones): 
	\begin{itemize}
	\item Extracción del dataset del fichero csv origen a un dataframe de Pandas, filtrado de las transacciones correspondientes al año 2016 y creación de una nueva variable llamada \textit{final\_date\_time} a partir de las columnas \textit{transaction\_date\_time} y \textit{duration\_mins}.
	\item Eliminación de transacciones con duración incorrecta (negativa o nula) que son menos de un 0,1\% del total.
	\item Transformación de las transacciones con distinto día de inicio y fin que son un poco más de un 0,1\% del total. Como para el análisis de ocupación sólo hay que tener en cuenta el rango de operación de los parquímetros (8-20h), es necesario duplicar las transacciones de larga duración para tener en cuenta los dos días, origen y final, de forma independiente, modificando sus fechas y horas para adaptarlas al rango de análisis. Así para la primera mitad de las transacciones duplicadas modificamos su fecha final para que coincida con la fecha inicial y su hora final a las 20h. Y para la segunda mitad de las transacciones duplicadas modificamos su fecha origen para que coincida con la fecha final y su hora origen a las 8h siempre que su hora final sea superior a esa hora, porque si es inferior borramos la segunda parte de la transacción duplicada. 
	\item Borrado de las transacciones existentes con hora inicial y final inferior a las 8h o con hora inicial y final posterior a las 20h. Aunque no tienen sentido desde el punto de vista de uso de los parquímetros, existen en el dataset y es necesario borrarlas, suponiendo más de un 0,65\% del total.
	\item Transformación de las horas iniciales y finales de las transacciones al rango horario de funcionamiento de los parquímetros. Para su posterior agregación redondeamos las horas eliminando los minutos y segundos y modificamos a las 08:00h las que tienen una hora inicial inferior y a las 20:00h las que tienen una hora final superior.
	\item Borrado de las transacciones realizadas por error en domingos. Aunque también consideramos inicialmente la eliminación de los días festivos, finalmente no lo hicimos para facilitar la predicción por parte de los modelos considerando que la serie puede tener una estacionalidad de lunes a sábado.
		\end{itemize}
Comprobamos que después de la extracción y transformación del dataset su tamaño se ha reducido en algo más de un 0.76\%.
\smallbreak
Sobre el \textbf{DATASET-2} (segmentos de calle - capacidad de plazas y distritos) destacamos:
	\begin{itemize}
	\item Extracción del dataset del fichero csv origen a un dataframe de Pandas con más de 13.700 registros. En este dataset observamos que hay bastantes valores nulos y que para un mismo valor de \textit{element\_key} hay distintos valores en la columna \textit{parking\_spaces}, por lo que decidimos quedarnos con el máximo valor de capacidad de plazas agrupando por \textit{element\_key}. Adicionalmente encontramos dos parquímetros cada uno con dos valores distintos de barrio-distrito de la ciudad. Necesitaremos utilizar la información del tercer dataset para poder discriminar cuál es el valor más adecuado.
	\item Eliminación de aquellos parquímetros con valores de capacidad de plazas igual a cero (sólo 6). Comprobamos que después de la extracción y transformación del dataset su tamaño se ha reducido a 1703 registros.
	\end{itemize}

Combinamos los primeros datasets para construir una serie espacio-temporal del porcentaje de ocupación de plazas de parking:
	\begin{itemize}
	\item Los tres primeros datasets comparten entre sí la variable \textit{element\_key}, habiendo 1514 valores distintos en el primer dataset, 1517 en el segundo y 1701 en el tercero. Al combinarlos entre sí en un nuevo dataframe observamos que disponemos de más de 10,6 millones de transacciones asociadas a 1443 parquímetros distintos.
	\item Creamos dos nuevos dataframes como copia del último generado, donde añadimos una nueva columna llamada \textit{timestamp\_sign}. En el primer dataframe consideramos sólo la fecha-hora (\textit{timestamp}) de inicio de la transacción y la nueva columna toma el valor 1, y en el segundo dataframe consideramos sólo la fecha-hora de fin de la transacción y la nueva columna toma el valor -1. Uniendo ambos dataframes, la variable \textit{timestamp\_sign} nos ayudará a calcular el número de plazas ocupadas para cada parquímetro y hora.
	\item Agrupando el dataframe anterior por \textit{element\_key} y \textit{timestamp}, creamos una nueva columna llamada \textit{occupation} calculada como la suma acumulada de la columna \textit{timestamp\_sign}. Luego eliminamos duplicados y nos quedamos con el último registro que es el que contiene la suma acumulada total y por tanto contabiliza las transacciones de inicio y fin registradas en una misma franja horaria. Agrupando luego por \textit{element\_key} y \textit{day\_year} (ordinal del día del año asociado al \textit{timestamp}), calculamos la suma acumulada total de la columna \textit{occupation} para obtener el total de plazas ocupadas para ese día y hasta esa hora. Por último convertimos el valor absoluto de ocupación en porcentaje dividiendo por el total de plazas disponibles.
	\item Observamos que tenemos casi un 5\% de registros con un porcentaje de ocupación superior al 100\% y que además no corresponde a casos puntuales sino que casi el 82\% de los parquímetros tiene algún registro en esa situación. Una vez que revisamos que no hay ningún error en la generación de las cifras de ocupación acumuladas y que las transacciones realizadas en el mismo día y tramo horario se contabilizan correctamente, el problema sólo es atribuible a la cifra de plazas de parking disponibles.
	\end{itemize}
Corrección de la capacidad de plazas de parking disponibles:
	\begin{itemize}
	\item Hemos encontrado en la web de SDOT otra API \cite{ETLtwo} que contiene el campo \textit{ELMNTKEY} asociado a otros campos con información de plazas de aparcamiento, aunque no hemos conseguido localizar información explicativa al respecto. Hemos creado un notebook de Python aparte llamado \textit{'ETL\_SDOT\_StreetParkingCategory.ipynb'} para realizar las consultas a esa API y descargar la información en varios ficheros json (como ya habíamos mencionado anteriormente todas las APIs de SDOT fijan un límite de respuesta de 1000 registros por consulta). De cada fichero json seleccionamos los parámetros que nos interesan y consolidamos los datos en un dataframe de Pandas que volcamos finalmente en un fichero csv llamado \textit{'Street\_Parking.csv'} (\textbf{DATASET-18}). Obtenemos más de 46 mil registros sin duplicidad de \textit{element\_key}. Para cada segmento de calle identificado por el \textit{element\_key} tenemos las siguientes variables: \textit{parking\_category}, \textit{parking\_spaces}, \textit{total\_zones}, \textit{total\_nopark} y \textit{total\_spaces}, donde la cifra de la columna \textit{total\_spaces} es la suma de las 3 variables anteriores.
	\item Eliminamos aquellos segmentos de calle con valores de capacidad de plazas igual a cero (97 registros).
	\item Comparamos los datos de la columna \textit{parking\_spaces} del nuevo dataset con el \textbf{DATASET-2}  y encontramos que hay un 62\% de coincidencias, que es un valor alto teniendo en cuenta que en el \textbf{DATASET-2} teníamos valores diversos de capacidad para un mismo segmento de calle y habíamos seleccionado el valor máximo de los disponibles. Dado que hemos encontrado un nuevo valor de capacidad (la variable \textit{total\_spaces} que engloba a la que teníamos), decidimos utilizarla a pesar de no conocer el significado de las otras 2 variables y del sospechoso nombre de una de ellas (\textit{total\_zones} y \textit{total\_nopark}).
	\item Recalculamos los porcentajes de ocupación considerando los nuevos valores de capacidad de plazas disponibles y observamos en este caso que tenemos sólo un 0,3\% de registros con un porcentaje de ocupación superior al 100\% y que además corresponden a sólo 194 segmentos de calle, por lo que procedemos a eliminarlos de la serie manteniendo su elevado tamaño total.
	\item Adicionalmente decidimos acotar la serie teniendo en cuenta los valores de la variable \textit{parking\_category}. Menos de un 7\% de los parquímetros corresponden a categorías especiales (\textit{No Parking Allowed}, \textit{Restricted Parking Zone} o \textit{Carpool Parking}) que pueden perjudicar el objetivo de generalización de la predicción de nuestro proyecto, por lo que decidimos quedarnos únicamente con la categoría mayoritaria.
	\end{itemize}
Completamos la serie con las horas intermedias faltantes:
	\begin{itemize}
	\item Observamos que es necesario completar la serie porque hay muchos casos en los que no tenemos transacciones en el primer dataset durante alguna franja horaria. Por ejemplo, para un parquímetro podemos tener transacciones en la franja de las 12h (de 12:00 a 12:59) y no tener transacciones nuevas hasta las 16h, por lo que al construir la serie nos faltan las franjas de las 13h, 14h y 15h que tendrán la misma ocupación que la franja de las 12h porque no ha habido transacciones en ese rango horario y por tanto no hay cambios.
	\item Observamos también que sólo 217 del total de 1110 parquímetros (menos del 20\%) tienen transacciones todos los hábiles del año. Pero no completamos esos casos porque en ese caso estaríamos falseando los datos.
	\item Obtenemos una serie espacio-temporal de más de 3,8 millones de registros.
	\end{itemize}
Añadimos variables adicionales a la serie combinándola con el resto de datasets mencionados en el apartado anterior:
	\begin{itemize}
	\item Combinamos la serie con el \textbf{DATASET-4} (información meteorológica diaria) mediante la variable \textit{day\_year} añadiendo a la serie las columnas de cantidad de precipitación diaria, temperatura máxima y temperatura mínima diaria.
	\item Añadimos el \textbf{DATASET-5} (sensor de temperatura más próxima por parquímetro) y \textbf{DATASET-6} (serie de medidas por hora de los sensores de temperatura) a la serie espacio-temporal, combinando el primer dataset por la variable \textit{element\_key} y el segundo por las variables \textit{timestamp} y \textit{station-closest}.
	\item Añadimos el \textbf{DATASET-15} (indicadores booleanos de proximidad de puntos de interés cultural y deportivo a cada parquímetro) a la serie combinando los datasets por la variable \textit{element\_key}.
	\item Con el \textbf{DATASET-16} (lista de día y coordenadas de eventos) creamos en la serie una nueva columna booleana que indica para cada transacción si ese día hay un evento o no, y si dicho evento además está próximo al parquímetro de cada transacción. Y como en casos anteriores la proximidad está calculada con la distancia Haversine y definida por un valor inferior a 75 metros.
	\item Combinamos la serie con el \textbf{DATASET-17} (parámetros de calidad del aire) mediante la variable día del año (columna \textit{day\_year}).
	\item Exportamos la serie a un fichero csv llamado \textit{'Serie\_Total2016\_ext.csv'}
	\end{itemize}
Y por último filtramos la serie global completa para seleccionar la información de la serie asociada a 30 parquímetros con los que realizamos la evaluación del mejor modelo de predicción. Para elegir los 30 parquímetros seleccionamos primero aquellos parquímetros con menos días del año sin transacciones. Y por otra parte seleccionamos también aquellos parquímetros que tienen un mayor número total de transacciones en el año. Con la intersección de esas dos selecciones obtenemos los 30 parquímetros elegidos para la evaluación de los modelos.



\chapter{Análisis Exploratorio de los Datos (EDA)}
En este capítulo realizamos un análisis exploratorio (EDA) de la serie espacio-temporal construida en el capítulo anterior. El objetivo es estudiar el \textit{dataset} en dos niveles, para encontrar sus características más relevantes y describir su estructura:
	\begin{enumerate}
		\item Análisis descriptivo estático, donde se estudian las covariables (variables que no son coordenadas espaciales o temporales y pueden describir o predecir el resultado), estableciendo relaciones entre ellas y extrayendo conclusiones con impacto en capítulos posteriores.
		\item Análisis descriptivo dinámico, donde se analiza la estructura temporal y espacial de los datos, describiendo sus principales parámetros y características.
	\end{enumerate} 

\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-dataset-head1}
		\includegraphics[scale=.5]{EDA-dataset-head2}
		\caption{Extracto de las primeras muestras de la serie espacio-temporal}
		\label{EDA-dataset-head}
	\end{figure}

El conjunto de datos bajo análisis consta de 22 columnas y 4.182.480 observaciones. En la Figura \ref{EDA-dataset-head} se muestra una captura de los cinco primeros registros. El dataset consta de dos columnas con coordenadas espaciales (\textit{latitude} y \textit{longitude}), una columna con coordenadas temporales (\textit{timestamp}), la variable a predecir (el porcentaje de ocupación, columna \textit{occupation\_perc}), y 15 covariables. 

	\section{Análisis descriptivo estático}
	La serie bajo estudio presenta las siguientes 9 covariables numéricas continuas:
	\begin{itemize}
		\item Temperatura máxima diaria
		\item Temperatura mínima diaria
		\item Cantidad de precipitación diaria
		\item Temperatura media horaria de la superficie del asfalto
		\item Temperatura media horaria ambiente
		\item Cantidad de dióxido de nitrógeno
		\item Cantidad de monóxido de carbono
		\item Cantidad de ozono
		\item Cantidad de partículas en suspensión de 2.5 micrómetros o menos
	\end{itemize}
	
	\subsection{Temperatura máxima diaria}
	La temperatura máxima registrada durante los días en los que se ha producido una transacción sigue la distribución que se muestra en la Figura \ref{EDA-dist-tmax} que es aproximadamente una distribución normal centrada en la media y con desviación típica la de la muestra. A lo largo de los días que recoge el dataset, la temperatura máxima media es de 17.14$ºC$, mientras que su desviación típica es 7.06$ºC$. 
		
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-tmaxC}
		\caption{Distribución de temperaturas máximas}
		\label{EDA-dist-tmax}
	\end{figure}

En la Tabla \ref{EDA-confint-tmax} se presentan los intervalos de confianza para la media y varianza de la temperatura máxima de la muestra ($\alpha=0.05$). Como puede observarse, el intervalo de confianza para ambas medidas es muy estrecho, debiéndose al tamaño elevado de la muestra.

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media (ºC)      & 17.136 & 17.150  \\
Desv. Est. (ºC) & 7.050 & 7.060  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la temperatura máxima}
\label{EDA-confint-tmax}
\end{table}

	
\subsection{Temperatura mínima diaria}
Las mismas conclusiones que se han presentado sobre la temperatura máxima pueden realizarse sobre la temperatura mínima. La temperatura media mínima es 8.74$ºC$, mientras que su desviación típica es 4.54$ºC$. En la Figura \ref{EDA-dist-tmin} se muestra la distribución estadística y en la Tabla \ref{EDA-confint-tmin} los intervalos de confianza para cada parámetro, calculados para $\alpha=0.05$.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.2]{EDA-dist-tminC}
		\caption{Distribución de temperaturas mínimas}
		\label{EDA-dist-tmin}
	\end{figure}

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media (ºC)      & 8.731 & 8.740  \\
Desv. Est. (ºC) & 4.536 & 4.542  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la temperatura mínima}
\label{EDA-confint-tmin}
\end{table}

De nuevo, debido al tamaño de la muestra, los valores de media y desviación típica calculados son muy precisos. Además, también puede asumirse que la distribución es normal.
La normalidad tanto de temperatura máxima como de temperatura mínima puede ayudar con el desarrollo de los modelos predictivos posteriores, debido a que muchas veces exigen normalidad en las variables que se utilizan para la predicción.		

	\subsection{Precipitaciones}
	La distribución de precipitaciones se muestra en la Figura \ref{EDA-dist-prcp}, y los intervalos de confianza para media y desviación típica en la Tabla \ref{EDA-confint-prcp}. Presenta una media de 0.13 pulgadas y una desviación típica muestral de 0.26 pulgadas. Al igual que en secciones anteriores, los intervalos de confianza son estrechos, como corresponde a una muestra de un gran número de datos. Sin embargo, la distribución en este caso no es gaussiana, principalmente debido a no ser simétrica. 

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.2]{EDA-dist-prcp-new}
		\caption{Distribución de precipitaciones}
		\label{EDA-dist-prcp}
	\end{figure}

	\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media       & 0.1273 & 0.1278  \\
Desv. Est.  & 0.2557 & 0.2561  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para las precipitaciones}
\label{EDA-confint-prcp}
\end{table}

Este parámetro tiene mucha dispersión, pues su coeficiente de variación ($C_V$), calculado como

\[ C_V = \dfrac{\sigma}{\bar{x}} \approx \dfrac{s}{\bar{x}},  \]

da como resultado $C_V=2$. En porcentaje, el coeficiente de variación es del 200\%, lo que implica que estamos ante una característica con gran variabilidad. 


\subsection{Temperatura media horaria del asfalto}
	La temperatura media horaria de la superficie del asfalto sigue la distribución que se muestra en la Figura \ref{EDA-dist-troad}. Su media es de 18.28$ºC$, mientras que su desviación típica es 9.16$ºC$. 
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-troad}
		\caption{Distribución de temperatura media horaria del asfalto}
		\label{EDA-dist-troad}
	\end{figure}
	
En la Tabla \ref{EDA-confint-troad} se presentan los intervalos de confianza para la media y varianza de la temperatura media del asfalto en la muestra ($\alpha=0.05$). Como en las variables anteriores el intervalo de confianza para ambas medidas es muy estrecho, debiéndose al tamaño elevado de la muestra.
	
\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media (ºC)      & 18.265 & 18.284  \\
Desv. Est. (ºC) & 9.154 & 9.167  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la temperatura media horaria del asfalto}
\label{EDA-confint-troad}
\end{table}

\subsection{Temperatura media horaria ambiente}
	La temperatura media horaria ambiente sigue la distribución que se muestra en la Figura \ref{EDA-dist-tair}. Su media es de 14.65$ºC$, mientras que su desviación típica es 6.39$ºC$. En la Tabla \ref{EDA-confint-tair} se presentan los intervalos de confianza para la media y varianza de la temperatura media ambiente en la muestra ($\alpha=0.05$). 
		
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-tair}
		\caption{Distribución de temperatura media horaria ambiente}
		\label{EDA-dist-tair}
	\end{figure}
	
\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media (ºC)      & 14.646 & 14.659  \\
Desv. Est. (ºC) & 6.389 & 6.398  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la temperatura media horaria ambiente}
\label{EDA-confint-tair}
\end{table}


\subsection{Dióxido de nitrógeno}
	La cantidad de dióxido de nitrógeno sigue la distribución que se muestra en la Figura \ref{EDA-dist-no}. Su media es de 56.65 $\mu$g/$m^3$, mientras que su desviación típica es 14.79 $\mu$g/$m^3$. 
		\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.2]{EDA-dist-no}
		\caption{Distribución de cantidad de dióxido de nitrógeno}
		\label{EDA-dist-no}
	\end{figure}
	
	En la Tabla \ref{EDA-confint-no} se presentan los intervalos de confianza para la media y varianza de la medida de dióxido de nitrógeno en la muestra ($\alpha=0.05$). 
\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media			      & 56.633 & 56.663  \\
Desv. Est.			& 14.779 & 14.800  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la cantidad de dióxido de nitrógeno}
\label{EDA-confint-no}
\end{table}

\newpage		 
\subsection{Monóxido de carbono}
	La cantidad de monóxido de carbono sigue la distribución que se muestra en la Figura \ref{EDA-dist-co}, que a diferencia de la variable anterior no es gaussiana. Su media es de 512.55 $\mu$g/$m^3$, mientras que su desviación típica es 194.61 $\mu$g/$m^3$. En la Tabla \ref{EDA-confint-co} se presentan los intervalos de confianza para la media y varianza de la medida de monóxido de carbono en la muestra ($\alpha=0.05$). 
		
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-co}
		\caption{Distribución de cantidad de monóxido de carbono}
		\label{EDA-dist-co}
	\end{figure}
	
\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media           & 512.354 & 512.743  \\
Desv. Est.      & 194.472 & 194.747  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la cantidad de monóxido de carbono}
\label{EDA-confint-co}
\end{table}
 

\subsection{Ozono}
	La cantidad de ozono sigue la distribución que se muestra en la Figura \ref{EDA-dist-ooo}, que es gaussiana. Su media es de 67.16 $\mu$g/$m^3$, mientras que su desviación típica es 16.31 $\mu$g/$m^3$. En la Tabla \ref{EDA-confint-ooo} se presentan los intervalos de confianza para la media y varianza de la medida de ozono en la muestra ($\alpha=0.05$). 
		
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-ooo}
		\caption{Distribución de cantidad de ozono}
		\label{EDA-dist-ooo}
	\end{figure}

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media           & 67.142 & 67.174  \\
Desv. Est.      & 16.294 & 16.317  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la cantidad de ozono}
\label{EDA-confint-ooo}
\end{table}


\subsection{Partículas en suspensión}
	La cantidad de partículas en suspensión de tamaño inferior a 2.5 micras sigue la distribución que se muestra en la Figura \ref{EDA-dist-pm}, que no es gaussiana. Su media es de 5.63 $\mu$g/$m^3$, mientras que su desviación típica es 2.95 $\mu$g/$m^3$. En la Tabla \ref{EDA-confint-pm} se presentan los intervalos de confianza para la media y varianza de la medida de partículas en suspensión en la muestra ($\alpha=0.05$). 
		
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.2]{EDA-dist-pm}
		\caption{Distribución de cantidad de partículas en suspensión}
		\label{EDA-dist-pm}
	\end{figure}

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media           & 5.623 & 5.629  \\
Desv. Est.      & 2.945 & 2.950  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para la cantidad de partículas en suspensión}
\label{EDA-confint-pm}
\end{table}


\newpage
\subsection{Porcentaje de ocupación}
Finalmente, se presenta la distribución del porcentaje de ocupación de los parquímetros, tanto gráficamente (Figura \ref{EDA-dist-ocup}) como con los intervalos de confianza para un 5\% de significación (Tabla \ref{EDA-confint-ocup}). La media del porcentaje de ocupación es de 18.45\%, mientras que la desviación típica es 16.10\%.  

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.2]{EDA-dist-ocup-new}
		\caption{Distribución del porcentaje de ocupación medio de los parquímetros}
		\label{EDA-dist-ocup}
	\end{figure}

La distribución no es gaussiana y está muy polarizada hacia los valores inferiores. Del mismo modo que en las variables anteriores, se puede comprobar que los intervalos de confianza son muy estrechos debido al gran número de muestras de que consta el dataset. 

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Parámetro       & 2.5\% & 97.5\% \\ \midrule
Media (\%)      & 18.437 & 18.469  \\
Desv. Est. (\%) & 16.086 & 16.108  \\ \bottomrule
\end{tabular}
\caption{Intervalo de confianza para el porcentaje de ocupación}
\label{EDA-confint-ocup}
\end{table}

Por otro lado, tiene una varianza considerable, especialmente en relación con la media, como consecuencia de la variabilidad de la disponibilidad de plazas de aparcamiento. Sin embargo, la variabilidad de esta característica no es tan grande como habíamos visto para la variable de precipitaciones: su coeficiente de variación es del 87\%.


\newpage
\subsection{Análisis de correlaciones entre las covariables y el target}
A continuación, se presenta un análisis de correlaciones entre las covariables y el target, que permitirá disponer de información más precisa y detallada sobre la serie. Además, debido a que muchos modelos espacio-temporales constan de una parte regresiva, se podrá utilizar el resultado obtenido para predecir con mejor precisión. 
\smallbreak
Para el análisis de correlaciones, dado que todas las variables númericas son continuas, se utiliza el coeficiente de correlación de Pearson, definido como 

\[ r_{X_1X_2} = \dfrac{E[(X_1-\bar{x}_1)(X_2-\bar{x}_2)]}{s_{X_1}s_{X_2}}. \]

La significación estadística de este valor se estudia mediante un test T (\textbf{PDTE poner referencia}), que determina si el valor calculado es significativamente distinto de cero. Para ello, se calcula el estadístico $T$,

\[ T = \frac{r}{\sqrt{1-r^2}} \cdot \sqrt{N-2}, \]

que se distribuye según una $t$ de Student de $N-2$ grados de libertad. El p-valor se calcula de forma bilateral, utilizando las tablas de la $t$ de Student, como la probabilidad de obtener un valor más extremo del estadístico $T$ que se ha calculado con la muestra dada. Es decir, 

\[ p = \text{Prob}(\left|t\right| \geq \left|T\right|) \]

Establecemos el nivel de significación ($\alpha$) en el 5\%, por lo que el p-valor deberá ser menor de 0.05 para que sea válido y el resultado tenga significación estadística. 
\smallbreak
Mantendremos la estructura original del test, tal y como está establecido en (\textbf{PDTE poner referencia}), pero debemos tener en cuenta que $N$ es muy grande, y por lo tanto:

\begin{itemize}
\item La $t$ de Student se podría aproximar a una distribución normal.
\item Los resultados saldrán muy significativos, pues el valor de $T$ será muy elevado, situándose muy a la derecha o muy a la izquierda de la distribución $t$, quedando muy lejos del valor crítico definido por $\alpha=0.05$. 
\end{itemize}

Los resultados se presentan en la Tabla \ref{EDA-corr-target}, donde se muestra tanto el coeficiente de correlación de Pearson (r) como el p-valor asociado a cada uno de ellos. No hay evidencia de que haya correlación entre las covariables y el target. Además, esta conclusión estadísticamente es bastante significativa, pues todos los p-valores calculados son menores que el intervalo de significación establecido ($p<0.05$).

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Correlación con el porcentaje de ocupación & $r$ & $p$-valor \\ \midrule
Temperatura máxima                         & 0.005& 2.79e-25 \\
Temperatura mínima                         & 0.006 & 2.34e-29 \\
Precipitación                            	 & -0.006 & 5.23e-35 \\
Temperatura asfalto horaria                & 0.027 & 0.0 \\
Temperatura ambiente horaria							 & 0.012 & 1.76e-120 \\
Dióxido de nitrógeno 											 & -0.004 & 6.3e-12 \\
Monóxido de carbono 											 & 0.003 & 4.13e-10\\
Ozono 																		 & 0.010 & 5.23e-93\\
Partículas en suspensión 									 & -0.001 & 1.4e-02\\
\bottomrule
\end{tabular}
\caption{Correlaciones entre las covariables y el target}
\label{EDA-corr-target}
\end{table}

\smallbreak
Como se comentó anteriormente, todos los resultados son significativos porque la muestra es muy grande. Por eso, aunque los coeficientes de correlación están próximos a cero, son estadísticamente distintos de cero, lo que es lógico teniendo en cuenta el tamaño de la muestra.
\smallbreak
También hemos analizado la correlación del porcentaje de ocupación con las variables espaciales (latitud y longitud) y las variables temporales (mes, día de la semana, día del año) y los coeficientes de correlación también están próximos a cero.
\smallbreak
La conclusión, por lo tanto, es que no hay evidencias de un grado de correlación alto entre las covariables y el target. Esto podría dificultar el análisis de tipo regresivo, puesto que no hay relaciones lineales directas entre las variables presentadas y el porcentaje de ocupación. Este aspecto se tiene en cuenta a la hora de realizar la evaluación del mejor modelo de predicción espacio-temporal, pues algunos permiten incluir regresores. 


\subsection{Análisis de correlaciones mutuas entre las covariables}
En esta sección, se repite el análisis anterior, pero para estudiar las posibles correlaciones entre cada una de las covariables. De esta forma, se analiza la posible existencia de multicolinealidad, que pudiera influir en la parte regresiva de algunos de los modelos espacio-temporales. Por otro lado, se puede determinar si existen variables que están tan relacionadas que en realidad pertenecen a la misma distribución, con lo que debe tenerse en cuenta para eliminar alguna de ellas. Los resultados del análisis de correlaciones mutuas entre las covariables se muestran en la Tabla \ref{EDA-corr-covar} y en la Figura \ref{EDA-corr-matrix}.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.3]{EDA-corr-matrix}
		\caption{Matriz de correlación de las covariables}
		\label{EDA-corr-matrix}
	\end{figure}

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Correlación mutua & $r$ & $p$-valor \\ \midrule
Temperatura máxima - Temperatura mínima 						& \textbf{0.879} & 0 \\
Temperatura máxima - Precipitaciones    						& -0.273 & 0 \\
Temperatura máxima - Temperatura asfalto 						& \textbf{0.873} & 0 \\
Temperatura máxima - Temperatura ambiente horaria   & \textbf{0.901} & 0 \\
Temperatura máxima - Dióxido de nitrógeno 					& 0.111 & 0 \\
Temperatura máxima - Monóxido de carbono						& -0.164 & 0 \\
Temperatura máxima - Ozono 													& 0.230 & 0 \\
Temperatura máxima - Partículas en suspensión 			& -0.076 & 0 \\
Temperatura mínima - Precipitaciones    						& -0.106 & 0 \\
Temperatura mínima - Temperatura asfalto						& \textbf{0.795} & 0 \\
Temperatura mínima - Temperatura ambiente horaria   & \textbf{0.867} & 0 \\
Temperatura mínima - Dióxido de nitrógeno 					& 0.086 & 0 \\
Temperatura mínima - Monóxido de carbono 						& -0.347 & 0 \\
Temperatura mínima - Ozono 													& 0.047 & 0 \\
Temperatura mínima - Partículas en suspensión 			& -0.319 & 0 \\
Precipitaciones - Temperatura asfalto 							& -0.301 & 0 \\
Precipitaciones - Temperatura ambiente horaria 			& -0.257 & 0 \\
Precipitaciones - Dióxido de nitrógeno 							& -0.006 & 0 \\
Precipitaciones - Monóxido de carbono 							& -0.025 & 0 \\
Precipitaciones - Ozono 														& -0.082 & 0 \\
Precipitaciones - Partículas en suspensión 					& -0.215 & 0 \\
Temperatura asfalto - Temperatura ambiente horaria 	& \textbf{0.934} & 0 \\
Temperatura asfalto - Dióxido de nitrógeno 					& -0.012 & 0 \\
Temperatura asfalto - Monóxido de carbono 					& -0.237 & 0 \\
Temperatura asfalto - Ozono 												& 0.215 & 0 \\
Temperatura asfalto - Partículas en suspensión 			& -0.142 & 0 \\
Temperatura ambiente horaria - Dióxido de nitrógeno & -0.034 & 0 \\
Temperatura ambiente horaria - Monóxido de carbono  & -0.267 & 0 \\
Temperatura ambiente horaria - Ozono 								& 0.149 & 0 \\
Temperatura ambiente horaria - Partículas en suspensión & -0.173 & 0 \\
Dióxido de nitrógeno - Monóxido de carbono 					& 0.525 & 0 \\
Dióxido de nitrógeno - Ozono 												& 0.202 & 0 \\
Dióxido de nitrógeno - Partículas en suspensión 		& 0.487 & 0 \\
Monóxido de carbono - Ozono 												& 0.046 & 0 \\
Monóxido de carbono - Partículas en suspensión 			& 0.671 & 0 \\
Ozono - Partículas en suspensión 										& 0.018 & 0 \\
\bottomrule
\end{tabular}
\caption{Correlaciones mutuas entre las covariables}
\label{EDA-corr-covar}
\end{table}

\newpage
De nuevo, debido al tamaño de la muestra, las conclusiones son muy significativas. Se puede observar que hay una correlación muy fuerte entre las cuatro variables de temperatura. Esta conclusión es lógica pues las cuatro variables forman parte de una misma información, si aumenta la temperatura media ambiente, por ejemplo en verano, suben tanto las temperaturas mínimas como las máximas (salvo casos extremos), y además con el mismo signo. Y la temperatura ambiente afecta directamente a la temperatura del asfalto. Sin embargo, las cuatro variables no provienen de la misma distribución estadística, pues su media es claramente diferente (algo menos entre la temperatura máxima y la temperatura media ambiente), y la muestra es suficientemente grande. Esta afirmación se demuestra mediante la aplicación del test de Kolmogorov-Smirnov (KS), que analiza las diferencias entre las dos funciones de distribución que se están comparando. Se calcula el estadístico $D$, como 

\[ D = \max[F_1(x)-F_2(x)], \]

donde $F_1$ y $F_2$ son las funciones de distribución de las dos variables bajo comparación. El resultado obtenido es que las funciones de distribución de las cuatro variables difieren en los valores de D que se muestran en la Tabla \ref{EDA-KS}, con una significación estadística altísima, de nuevo debido al tamaño de la muestra. 

\begin{table}[!htb]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Variables & $D$ & $p$-valor \\ \midrule
Temperatura máxima - Temperatura mínima & 0.54 & 0 \\
Temperatura máxima - Temperatura media asfalto & 0.11 & 0 \\
Temperatura máxima - Temperatura media ambiente & 0.15 & 0 \\
Temperatura mínima - Temperatura media asfalto & 0.54 & 0 \\
Temperatura mínima - Temperatura media ambiente & 0.43 & 0\\
Temperatura media asfalto - Temperatura media ambiente & 0.21 & 0 \\
\bottomrule
\end{tabular}
\caption{Test de Kolmogorov-Smirnov para las variables de temperatura}
\label{EDA-KS}
\end{table}

Los mismos resultados pueden observarse en la Figura \ref{EDA-CDF-new}, donde se aprecian que las diferencias máximas entre las funciones de distribución de la temperatura máxima, temperatura media ambiente y del asfalto son muy pequeñas (entre 0.1-0.2 aproximadamente). 
 
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.3]{EDA-CDF-new}
		\caption{Funciones de distribución de las variables de temperatura}
		\label{EDA-CDF-new}
	\end{figure}

Este test se presenta únicamente como comprobación formal de lo que se comentó con anterioridad: la temperatura máxima y la mínima están muy relacionadas, pero no provienen de la misma distribución, por lo que tendrán impactos diferentes en el porcentaje de ocupación. Por ejemplo, es posible que las temperaturas mínimas no afecten del mismo modo a los desplazamientos y ocupación de los parquímetros en Seattle que las temperaturas máximas. 
\smallbreak
	
	\section{Análisis descriptivo dinámico}
	A continuación se presenta un análisis dinámico del dataset, donde se describen efectos y propiedades del mismo, pero en función del lugar y tiempo en el que se produjeron. Se realiza primero un estudio temporal, donde se relaciona la variable de ocupación (y la distribución de transacciones) con la temporalidad del fenómeno bajo estudio. Después, se analiza de forma geográfica, presentando las distribuciones de ocupación por localización (parquímetro). Por último, se explican cuestiones relativas a la frecuencia de actualización de los parquímetros, que es muy relevante a la hora de decidir qué parquímetros utilizar para realizar la predicción. 
	
	\subsection{Análisis temporal}
	En primer lugar, en la Figura \ref{EDA-dist-tickets} se muestra una representación gráfica en la que aparece la distribución estadística de las transacciones (tickets) en función de las horas en las que se produjeron (inicio y fin). Cabe destacar que las horas centrales del día (11h-13h) son las de mayor actividad para el inicio del ticket, y para la hora de fin además de la última hora (19h) también destaca el rango entre las 13-15h.
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.45]{EDA-dist-tickets-ini}
		\includegraphics[scale=.45]{EDA-dist-tickets-fin}
		\caption{Distribución de la hora de inicio y de la hora de fin de las transacciones}
		\label{EDA-dist-tickets}
	\end{figure}	
	
	Continuando con el análisis, la Figura \ref{EDA-ocupacion-horas} presenta la distribución del porcentaje de ocupación de los parquímetros en función de la hora del día. Se observa que las horas centrales del día suelen constituir las horas más relevantes para el análisis.
	
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-ocupacion-horas-new}
		\caption{Distribución de la ocupación de los parquímetros en función de la hora del día}
		\label{EDA-ocupacion-horas}
	\end{figure}
		
	Dentro de una misma semana, tiende a haber mayor ocupación en los días finales de la semana (Jueves, etiquetado como 3, Viernes, etiquetado como 4, y Sábado, etiquetado como 5), según se representa en la Figura \ref{EDA-ocupacion-dia}. Es lógico que se obtenga este resultado, pues los días cercanos al fin de semana suelen llevar aparejados mayores desplazamientos. Nótese que el domingo no aparece representado por no estar activo el sistema de pago por aparcamiento en domingos y días festivos. 
		
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-ocupacion-dia-new}
		\caption{Distribución de la ocupación de los parquímetros en función del día de la semana}
		\label{EDA-ocupacion-dia}
	\end{figure}
	
	Si extendemos el análisis a los días dentro de un mes (Figura \ref{EDA-ocupacion-diames}), se observa que la distribución es relativamente uniforme: no se aprecia una diferencia significativa entre los días de principio de mes (días 1 a 7, etiquetados como 'begin'), los días de final de mes (días 25 a 31, etiquetados como 'end'), y el resto (etiquetados como 'rest'). Además, también se aprecia que la distribución por horas se mantiene tanto a lo largo de una semana como a lo largo de un mes, siempre observando ocupaciones mayores en las horas centrales del día. 
	
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-ocupacion-diames-new}
		\caption{Distribución de la ocupación de los parquímetros en función del día del mes}
		\label{EDA-ocupacion-diames}
	\end{figure}
	
	Y si consideramos un año completo, vemos que no hay gran diferencia entre los meses y se sigue manteniendo la tendencia horaria de mayores ocupaciones entre las 10 y las 14h. Se aprecia que en los meses de verano hay mayor ocupación que a lo largo del resto del año, posiblemente debido a la influencia de temperaturas más suaves, mientras que en las horas de ocupación mayor también destacan los meses de Febrero y Marzo (Figura \ref{EDA-ocupacion-mes}).
	
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-ocupacion-mes-new}
		\caption{Distribución de la ocupación de los parquímetros en función del mes}
		\label{EDA-ocupacion-mes}
	\end{figure}

A continuación observamos la variabilidad de la distribución de ocupación para aquellos parquímetros con mayores porcentajes de ocupación en media que se presentan en la Figura \ref{EDA-dist-parq-new}: 

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{EDA-dist-parq-new}
		\caption{Distribución de la ocupación de los parquímetros con mayor porcentaje medio ($>35\%$)}
		\label{EDA-dist-parq-new}
	\end{figure}

Y por último la distribución de ocupación para los 100 parquímetros con mayor número de transacciones:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.35]{EDA-dist-top100-new}
		\caption{Distribución de la ocupación de los 100 parquímetros con más transacciones}
		\label{EDA-dist-top100-new}
	\end{figure}

	
\subsection{Análisis espacial}
En cuanto a la distribución espacial de la ocupación de los parquímetros teniendo en cuenta el distrito al que pertenecen, puede observarse en la Figura \ref{EDA-ocupacion-distrito} que es muy heterogénea. Hay parquímetros con una tasa de ocupación elevada durante gran parte del día, con picos altos en el rango de 18-19h, y parquímetros que apenas se llenan durante todo el día. 

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-ocupacion-distrito}
		\caption{Distribución de la ocupación de los parquímetros según su distrito}
		\label{EDA-ocupacion-distrito}
	\end{figure}

En la Figura \ref{EDA-distritos} puede observarse la ubicación de los parquímetros contenidos en la serie y agrupados por colores identificando los distintos distritos a los que pertenecen:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.37]{EDA-distritos}
		\caption{Mapa de los parquímetros por distritos}
		\label{EDA-distritos}
	\end{figure}

			
\subsection{Transacciones diarias por parquímetro}
En el año 2016 tenemos 304 días hábiles para el uso de los parquímetros (sin domingos y festivos). Calculamos el número medio de transacciones por día para los 1110 parquímetros existentes en la serie y observamos que sólo para 217 parquímetros hay transacciones todos esos días hábiles. En la Figura \ref{EDA-box-tbd} vemos la distribución del número medio de transacciones por día. El 25\% de los parquímetros tiene en media menos de 1 transacción por hora, el 60\% de los parquímetros tiene en media menos de 2 transacciones por hora y sólo el 5\% de los parquímetros tiene más de 4 transacciones por hora. Tenemos por tanto parquímetros con alto número de transacciones que ven circular muchos vehículos por ellos durante el día junto a parquímetros que  prácticamente no tienen movimiento. Disponer de muchas transacciones da información sobre el fenómeno bajo estudio, por lo que seleccionaremos parquímetros de ese tipo que nos permitan realizar buenas generalizaciones.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.5]{EDA-box-tbd}
		\caption{Diagrama de caja asociado al número medio de transacciones diarias de los parquímetros}
		\label{EDA-box-tbd}
	\end{figure}	

Tomando el parquímetro con id 12289, que de los 30 parquímetros seleccionados para la evaluación de los modelos de predicción es el que tiene menor número de valores de ocupación igual a 0, en la Figura \ref{EDA-parq12289-sem} se presenta la variación del porcentaje de ocupación en función del tiempo durante la primera semana del año y en la Figura \ref{EDA-parq12289-mes} durante el primer mes: 
	
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{EDA-parq12289-sem}
		\caption{Porcentaje de ocupación del parquímetro 12289 durante la primera semana del año}
		\label{EDA-parq12289-sem}
	\end{figure}	

	\begin{center}
		\includegraphics[scale=.4]{EDA-parq12289-mes}
		\captionof{figure}{Porcentaje de ocupación del parquímetro 12289 durante el mes de Enero}
		\label{EDA-parq12289-mes}
	\end{center}	
	
Se observa que no hay una estacionalidad clara en las gráficas, por lo que a priori parece no es sencillo que los modelos consigan realizar una predicción muy exacta. 
	
	
\chapter{Selección de Variables}

Este capítulo presenta un análisis referente a la elección de las mejores variables para alimentar el proceso de entrenamiento y predicción de los modelos de capítulos posteriores. Se llevan a cabo dos análisis independientes, desde dos puntos de vista diferentes. El primer análisis estudia, mediante procedimientos estadísticos, las correlaciones y asociaciones que existen entre las covariables (regresores exógenos) y el \emph{target}. Para ello, se utilizan diferentes técnicas estadísticas. Por otro lado, se realiza también un análisis basado en \emph{Machine Learning}, observando la importancia que las diferentes covariables tienen a la hora de ajustar un modelo estático. 

\section{Análisis estadístico de la importancia de las variables} 
A la hora de realizar una predicción mediante cualquier modelo predictivo, es de crucial importancia realizar una selección previa de las variables. Esto se debe a que el potencial predictivo de un conjunto de datos depende directamente de las distintas características que introduzca. Un dataset con pocas variables no podrá realizar predicciones complejas y precisas, mientras que un dataset con demasiadas variables tendrá dificultades para generalizar el resultado obtenido (sobreajuste). Normalmente, una combinación de las variables del dataset que deje fuera algunas de las características con menos importancia dará un resultado con la suficiente complejidad como para ser útil, y por otro lado será capaz de generalizar el resultado a muestras que no se encuentren en el conjunto de entrenamiento.
\smallbreak 
Esta primera sección del capítulo explora las relaciones que existen entre las distintas variables del dataset de parquímetros de Seattle, con el objetivo de ver, exclusivamente, cómo se relacionan los distintos regresores exógenos con la variable de predicción (porcentaje de ocupación de un parquímetro en concreto, a una hora determinada). En el capítulo de EDA ya se han estudiado algunos parámetros similares a lo que se muestra en este capítulo, si bien el objetivo aquí es definir un conjunto de variables para alimentar a los modelos predictivos de capítulos posteriores. 
\smallbreak
A lo largo de este capítulo se hace referenica en varias ocasiones a la \textbf{prueba U de Mann-Whitney}. Esta prueba, entre otras aplicaciones, se utiliza para establecer si hay asociación entre una variable continua (el porcentaje de ocupación en nuestro caso) y una variable binaria. La prueba U de Mann-Whitney es un test no paramétrico (no impone ninguna condición a la distribución de los datos), basado en suma de rangos. En ella, se calcula el estadístico $U$, definido como:

\[ U = \min(U_1, U_2) \]

$U_1$ y $U_2$ se calculan como sigue:

\[ U_i = R_i -\dfrac{n_i(n_i+1)}{2}, \; i \in \{1,2\} \text{ ,} \]

donde $n_1$ es el número de muestras que corresponden a uno de los dos valores de la variable binaria bajo estudio, mientras que $n_2$ es el número de muestras restantes. $R_i$ es la suma de los rangos correspondiente a cada muestra. 

\subsection{Relevancia de los intervalos horarios}
A continuación, se muestran los resultados de aplicar la prueba U a los 30 Element Key seleccionados para análisis. Para ello, se segmenta el dataset por horas, definiendo la hipótesis nula como sigue:
\[ 
H_0: \text{encontrarse en el rango horario } h_i \; \text{tiene la misma distribución estadística 
que no encontrarse en él}
\]

Los resultados se muestran en las Figuras \ref{topEK0-5} - \ref{topEK25-30}, e indican que, para un nivel de significación del 99.5\%, la gran mayoría de los intervalos horarios son significativos, aunque la distribución concreta depende de cada Element Key.

\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK0-5_h}
		\caption{Relevancia horas top 30 EK (1 de 6)}
		\label{topEK0-5}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK5-10_h}
		\caption{Relevancia horas top 30 EK (2 de 6)}
		\label{topEK5-10}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK10-15_h}
		\caption{Relevancia horas top 30 EK (3 de 6)}
		\label{topEK10-15}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK15-20_h}
		\caption{Relevancia horas top 30 EK (4 de 6)}
		\label{topEK15-20}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK20-25_h}
		\caption{Relevancia horas top 30 EK (5 de 6)}
		\label{topEK20-25}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK25-30_h}
		\caption{Relevancia horas top 30 EK (6 de 6)}
		\label{topEK25-30}
	\end{figure}

\newpage	
\subsection{Relevancia del día de la semana}
En esta sección se lleva a cabo un análisis equivalente al de la sección anterior, pero estudiando la significación estadística del día de la semana en el que se realiza la predicción. Para ello, se utiliza la prueba U de Mann-Whitney, pues estamos comparando de nuevo una variable con varios niveles y una variable continua. En este caso, los intervalos corresponden con el día de la semana: Lunes (L), Martes (L), Miércoles (X), Jueves (J), Viernes (V) y Sábado (S). Los domingos quedan fuera del análisis al ser días no operativos de los parquímetros. Los resultados se presentan en las Figuras \ref{topEK0-5w} - \ref{topEK25-30w}. Del mismo modo que en la sección anterior, se presentan gráficos de barras donde se puede observar el nivel de significación asociado a cada día de la semana y a cada EK. El código de colores es idéntico al de la sección anterior. 

\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK0-5_w}
		\caption{Relevancia weekday top 30 EK (1 de 6)}
		\label{topEK0-5w}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK5-10_w}
		\caption{Relevancia weekday top 30 EK (2 de 6)}
		\label{topEK5-10w}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK10-15_w}
		\caption{Relevancia weekday top 30 EK (3 de 6)}
		\label{topEK10-15w}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK15-20_w}
		\caption{Relevancia weekday top 30 EK (4 de 6)}
		\label{topEK15-20w}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK20-25_w}
		\caption{Relevancia weekday top 30 EK (5 de 6)}
		\label{topEK20-25w}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=.25]{topEK25-30_w}
		\caption{Relevancia weekday top 30 EK (6 de 6)}
		\label{topEK25-30w}
	\end{figure}
	
	La relevancia del día de la semana es menor que la de la hora del día, pues hay más columnas en gris para el día de la semana. Aun así, los resultados muestran que el día de la semana es relevante para la predicción, aunque menos que la hora del día.
	
\newpage	
\subsection{Regresores exógenos}
Esta sección analiza, mediante varios métodos, la importancia de los regresores exógenos del dataset. Dado que hay distintos tipos de variables, realizaremos una prueba diferente para cada una de ellas. En concreto:
\begin{itemize}
	\item Variables continuas: se compara una variable continua con el target (variable continua). Test: Correlación de Pearson
	\item Variables binarias: se compara una variable binaria con el target (variable continua). Test: U Mann-Whitney
\end{itemize}

\subsubsection{Variables continuas}
Son lecturas de sensores acerca de las condiciones atmosféricas y de temperatura. Se les aplica un test de correlación de Pearson con respecto al porcentaje de ocupación (target). Utilizaremos las siguientes reglas para calibrar el significado del coeficiente de correlación ($\rho$):

\begin{itemize}
 \item $0.0\leq \left|\,\rho\,\right|<0.3$: correlación \textit{débil}
 \item $0.3\leq \left|\,\rho\,\right| < 0.7$: correlación \textit{moderada} 
 \item $0.7\leq \left|\,\rho\,\right| \leq 1.0$: correlación \textit{fuerte }
\end{itemize}
Si la correlación es moderada o fuerte, el signo de $\rho$ nos indicará el sentido de la correlación (directa o inversa). El nivel de significación se establece en $\alpha=0.05$. En la Figura \ref{exo1037}, se presenta un gráfico de barras con los resultados del cálculo de la correlación para un Element Key en concreto. Se marcan en gris las variables cuya significación asintótica es menor que el nivel de significación.	

\begin{figure} [!h]
		\centering
		\includegraphics[scale=0.6]{exo1037}
		\caption{Regresores exógenos EK1037}
		\label{exo1037}
	\end{figure}
	
	En las Figuras \ref{correlation} y \ref{p-correlation} se muestra la distribución del coeficiente de correlación de Pearson de cada uno de los regresores exógenos con el porcentaje de ocupación, a lo largo del dataset formado por los 30 EK seleccionados. Además, se representa también su significación asintótica, para calibrar el nivel de significación que tiene el resultado. Como conclusión se establece que no existe una correlación demasiado elevada para ningún regresor exógeno. Esto se debe a la complejidad del fenómeno bajo estudio (distribución del flujo de personas que buscan sitio en un parquímetro). Solo existe un grado de significación compatible con $\alpha=0.05$ para algunas de las variables, que son aquellas para las que, en la Figura \ref{p-correlation} tienen  un pico en la distribución cerca de 0.
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=0.37]{correlation}
		\caption{Distribución del coeficiente de correlación de Pearson en el top 30}
		\label{correlation}
	\end{figure}
	
	\begin{figure} [!h]
		\centering
		\includegraphics[scale=0.37]{correlation-p}
		\caption{Significación asintótica de la correlación del top 30}
		\label{p-correlation}
	\end{figure}
	
	\subsection{Variables binarias}
	Este análisis únicamente es relevante si se tienen en cuenta varios Element Keys. Si solo se analiza un EK aislado, entonces todas estas variables son constantes y por tanto no aportan información. Por ello, para este cálculo, se consideran todos los EK. Tomando el dataset de Element Keys seleccionados, solo es distinta de cero la variable 'Punto de interés'. Además, esta variable obtiene un nivel de significación del orden de $10^{-53}$ bajo una prueba U de Mann-Whitney, lo que la convierte en una variable relevante, pero, tal y como se ha comentado, solo en el caso de que se consideren localizaciones geográficas diferentes. 

\newpage	
	\section{Importancia de variables basada en métodos de Machine Learning}
En esta sección se presenta un análisis de la importancia de las variables según métodos de Machine Learning. Para ello, se utiliza el concepto de \textbf{importancia de permutación}. Este concepto consiste en evaluar la diferencia que existe en la métrica utilizada para el análisis (AUC, precisión, ...) cuando se elimina una de las variables del dataset. Tras llevar a cabo este análisis, se obtienen diferentes pesos para cada una de las variables, clasificándose en términos de su importancia. Debe tenerse cuidado con este método para evaluar la importancia pues, en primer lugar, necesita de un modelo entrenado que dé buenos resultados y, en segundo lugar, es vulnerable a la multicolinealidad. 
\smallbreak
Por otro lado, es de crucial relevancia mencionar que la interpretación de los resultados de este análisis debe realizarse con cuidado. Esto se debe a que la estructura de los datos bajo análisis es de tipo serie espacio-temporal. Es decir, cada registro está relacionado con el anterior. Por lo tanto, al entrenar los modelos de ML, y separarlos en entrenamiento y validación, se va a romper la estructura de serie de los datos, con lo que el rendimiento del modelo puede ser bajo. Con ello, los resultados obtenidos no van a ser generalizables en sentido estricto, pero sí se va a obtener una medida de cómo son de relevantes las variables, similar a una correlación, pero basándonos en modelos más complejos que el coeficiente de Pearson. Para evaluar la importancia de permutación, se ha utilizado la implementación de ELI5, que dispone de un \textit{wrapper} para los modelos entrenados con \textit{sklearn}. Se han entrenado dos modelos, basados en Random Forest y Gradient Boosting respectivamente.

\newpage
\subsection{Entrenamiento de los modelos}
El entrenamiento de los modelos se ha realizado según los siguientes pasos:
\begin{enumerate}
\item Discretización de la variable target (porcentaje de ocupación) en 5 intervalos, para que encaje con los modelos de clasificación aceptados por la implementación de ELI5. El número de intervalos se ha obtenido de un compromiso entre rendimiento del modelo y capacidad de cómputo.
\item Discretización de la variable (latitud, longitud) en 25 cuadrantes. 
\item Eliminación del identificador de cada EK.
\item Separación en entrenamiento y validación.
\item Fitting.
\item Análisis de resultados.
\end{enumerate}

El modelo basado en Random Forest se ha entrenado con 75 estimadores, sin limitación en la profundidad de los nodos ni en su número de hojas. Del mismo modo, el modelo basado en Gradient Boosting utiliza también 75 árboles, sin penalizar la profundidad de los mismos. Aunque el hecho de no limitar la profundidad de los árboles puede llevar a sobreajustar el conjunto de datos, en este caso se ha realizado así para aumentar la capacidad predictiva de ambos modelos. Los resultados se evalúan mediante el ratio de verdaderos positivos (TPR) y el de falsos negativos (FPR), presentándolos en la curva ROC, y calculando el área bajo la curva (AUC), en la Figura \ref{roc-perm}. Los dos modelos tienen, en concreto, el valor de AUC que se 
muestra en la Tabla \ref{perm_t}.

\begin{table}[!htb]
\centering
\begin{tabular}{ll}
\hline
\textbf{Modelo}   & \textbf{AUC} \\ \hline
Random Forest     & 0.57         \\
Gradient Boosting & 0.54         \\ \hline
\end{tabular}
\caption{AUC modelos Permutation Importance}
\label{perm_t}
\end{table}

\begin{figure} [!h]
		\centering
		\includegraphics[scale=0.5]{roc_perm}
		\caption{Validación de los modelos utilizados para la importancia de permutación}
		\label{roc-perm}
	\end{figure} 
	
Se observa que el modelo basado en Random Forest es algo superior al de Gradient Boosting, con los parámetros seleccionados para su entrenamiento. Sin embargo, los dos modelos tienen dificultades para superar el valor de 0.5, que correspondería a una clasificación aleatoria. Esto se debe a lo que se comentó previamente acerca de la estructura de serie de los datos.

\newpage
\subsection{Permutation Importance}
Para concluir esta sección, se muestran los pesos de las variables más relevantes según los dos modelos estudiados. Estos resultados se presentan en la Figura \ref{weights}. 

\begin{center}
		\includegraphics[scale=0.5]{permw}
		\captionof{figure}{Pesos importancia de permutación}
		\label{weights}
	\end{center} 
	
	
\chapter{Descripción y aplicación de modelos predictivos}
Que un hecho o cantidad sea predecible depende de varios factores principales \cite{MODone}:
		\begin{itemize}
		\item cuántos datos hay disponibles
		\item cómo de bien entendemos los factores que contribuyen al hecho o cantidad
		\item si las predicciones pueden afectar al resultado de lo que se intenta predecir
		\end{itemize}
\smallbreak		
		Buenas predicciones son las que capturan los patrones y las relaciones que existen en los datos históricos pero no replican los eventos pasados que no volverán a ocurrir.
Todos los entornos son cambiantes, pero un buen modelo de predicción capta el modo en que las cosas cambian, asumiendo habitualmente que el modo en que el entorno cambia continuará en el futuro.
\smallbreak
Los métodos de predicción de series temporales más simples son aquellos que usan información sólo de la variable que se predice, sin tener en cuenta los factores externos que pueden afectar a su comportamiento.
En aquellos métodos más avanzados que lo permiten, hemos considerado también la opción de incluir variables adicionales externas como regresores, seleccionando tres variables: la temperatura del asfalto y la temperatura mínima, porque son dos de las variables destacadas por el análisis de importancia comentado en el capítulo anterior, y añadimos el día de la semana como variable \textit{dummie}.
Y adicionalmente hemos considerado la transformación logarítmica de la serie para comparar el rendimiento en la predicción de los modelos. La transformación logarítmica es popular en análisis de series temporales porque estabiliza la varianza, aunque no por ello está asegurado que se mejoren las predicciones. En nuestro caso hemos comparado el rendimiento en la predicción de algunos de los modelos considerando la serie original y la serie transformada logarítmicamente.
\smallbreak
\underline{Estacionalidad múltiple}: Una serie temporal presenta efectos estacionales si el comportamiento de la serie es parecido en ciertos tramos de tiempo periódicos en el tiempo.  Una serie con datos horarios como la nuestra suele tener típicamente 3 tipos de estacionalidad: diaria, semanal y anual. Hemos acotado el análisis al primer trimestre del año 2016 para que nuestros equipos informáticos puedan realizar los cálculos en plazos de tiempo razonables, por lo que nuestra serie tiene dos estacionalidades, diaria y semanal, con la particularidad de que el periodo diario está acotado a 12 horas y el periodo semanal a 6 días, por los horarios de funcionamiento de los parquímetros.
Para tratar con este tipo de series en los que hay varios tipos de estacionalidad, utilizamos la clase \textit{msts} de R que nos permite especificar todas las frecuencias que son relevantes e incluso admite frecuencias no enteras.
\smallbreak
En la Figura \ref{MOD-msts1037-37177} se muestra para dos parquímetros distintos el desglose de su serie, acotada temporalmente al primer trimestre del año, con las dos estacionalidades mencionadas (diaria y semanal). Se observa en ambos casos una periodicidad dinámica y evolutiva en el comportamiento de las dos gráficas de estacionalidad y también en la de tendencia. Destacamos también la diferencia de comportamiento de la serie entre los dos parquímetros seleccionados al azar y que se puede intuir a partir de las gráficas que será complejo conseguir alcanzar unas buenas aproximaciones en las predicciones.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.37]{MOD-msts1037-37177}
		\caption{Doble estacionalidad de la serie para dos parquímetros distintos (ids 1037 y 37177)}
		\label{MOD-msts1037-37177}
	\end{figure}

\section{Modelos considerados}
\subsection{Auto-arima}
\cite{MODtwo} El modelo \textit{auto.arima} ajusta la serie combinando valores de p, d y q y selecciona el mejor modelo \textit{ARIMA} que tiene el menor estadístico AIC corregido.
Hemos comprobado si la predicción mejoraba estableciendo un lambda distinto al valor NULL por defecto, definiendo \textit{lambda = ``auto``} para que se seleccione automáticamente una transformación Box-Cox.
Las transformaciones Box-Cox se basan en la siguiente función para $\lambda \neq 0$:

	\[
	f_\lambda(x)  = \frac{x^\lambda -1}{\lambda}
	\] 
\smallbreak
Si $\lambda$ = 0 entonces la función sería:
\smallbreak
	\[
	f_0(x)  = log(x)
	\]

\smallbreak
Para efectuar esta prueba hemos seleccionado el element key cuya serie tiene menos valores igual a 0 de porcentaje de ocupación, id 12289, y hemos acotado el análisis al primer trimestre. Los resultados del MAE obtenido son los siguientes:
\smallbreak
		
		\begin{table}[!htb]
		\centering
		\begin{tabular}{@{}lll@{}}
		\toprule
		Min.       & 7.919  \\ 
		1st Qu.    & 11.075  \\
		Median     & 13.819  \\
		Mean       & 17.521  \\
		3rd Qu.    & 17.450  \\ 
		Max.			 & 52.333 \\ \bottomrule
		\end{tabular}
		\end{table}
		
\smallbreak
En contraposición con el que obteníamos dejando lambda=NULL:

		\begin{table}[!htb]
		\centering
		\begin{tabular}{@{}lll@{}}
		\toprule
		Min.       & 7.399  \\ 
		1st Qu.    & 10.884  \\
		Median     & 12.916  \\
		Mean       & 13.122  \\
		3rd Qu.    & 15.334  \\ 
		Max.			 & 19.947 \\ \bottomrule
		\end{tabular}
		\end{table}
		
El MAE empeora por lo que optamos por dejar el valor por defecto y no proceder a ninguna transformación Box-Cox en este punto. Y seleccionamos \textit{seasonal=TRUE} porque tenemos una serie multiestacional definida con \textit{msts}.
\smallbreak
El modelo \textit{auto.arima} permite incluir variables externas como regresores, por lo que hemos utilizado las 3 variables antes mencionadas (temperatura del asfalto, temperatura mínima y día de la semana).
Al aplicar la función predictiva \textit{forecast} nos hemos encontrado con un problema explicado en la documentación de la función; cuando a un modelo creado con \textit{arima} se le especifican regresores, la función \textit{forecast} ignora el periodo de predicción (h=12 horas siguientes), por lo que consideramos la aplicación de un tipo de predicción conocida como \textit{ex-post}, ya que utilizamos información de los predictores externos en instantes de tiempo posteriores al momento de la predicción. En estos casos las predicciones no son válidas por sí mismas pero si son útiles como en nuestro caso para estudiar el comportamiento de los modelos de predicción en relación a los regresores.
\smallbreak
Además de comparar los modelos \textit{auto.arima} con y sin regresores para la serie original, hemos considerado la transformación logarítmica de la serie también en ambos casos, por lo que tenemos un total de 4 modelos \textit{auto.arima} por cada parquímetro. 

	\subsection{Medias Móviles}
La base del modelo de \textit{medias móviles (Moving-Average)} es que asume que las desviaciones actuales de los valores medios aritméticos de la serie dependen de las desviaciones anteriores.
El orden que se le fija a la media móvil determinará la desviación de la estimación y, cuanto más alto sea este valor, más homogénea será la desviación.
Nosotros hemos fijado el orden en 3, lo que significa que para la predicción del periodo n+1, el modelo tendrá en cuenta los valores de n, los de n-1 y los de n-2, calculando la media de ellos.
\smallbreak
Este modelo no admite regresores. Hemos aplicado el modelo sobre la serie original y sobre la serie transformada logarítmicamente.

	\subsection{MSTL}
\textit{STL} es un modelo versátil y robusto para descomponer series temporales. \textit{STL} es un acrónimo de \textit{"Seasonal and Trend decomposition using Loess"}, siendo \textit{Loess} un método de estimación de relaciones no lineales. \cite{MODone}
\textit{STL} tiene varias ventajas sobre otros métodos clásicos de descomposición:
	\begin{itemize}
	\item maneja cualquier tipo de estacionalidad, no sólo mensual o trimestral
	\item la componente de estacionalidad puede variar con el tiempo
	\item es robusto frente a outliers (valores atípicos) para las estimaciones de tendencia y estacionalidad aunque estos outliers afectarán a la componente residual de la serie
	\end{itemize}
Utilizamos el modelo \textit{MSTL} que es una variación de \textit{STL} para series con estacionalidad múltiple. Utilizando la función \textit{mstl} de R (también específica para series con estacionalidad múltiple), sobre la serie construida con la función \textit{msts}, obtenemos las gráficas de las dos estacionalidades indicadas (diaria y semanal), la tendencia (que se observa evolutiva) y el componente residual.
\smallbreak
Este modelo no admite regresores. Hemos aplicado el modelo sobre la serie original y sobre la serie transformada logarítmicamente.

	\subsection{BATS y TBATS}
\textit{BATS} es otro modelo cuyo acrónimo destaca las características más relevantes \cite{MODone}:
	\begin{itemize}
	\item B: transformaciones Box-Cox, mencionadas anteriormente, que arreglan problemas de normalidad y heterocedasticidad, es decir, no homogeneidad de varianzas
	\item A: errores ARMA (modelo de medias móviles autoregresivos para la estimación del componente residual de la serie)
	\item T: Tendencia
	\item S: estacionalidad (Seasonality)
	\end{itemize}
Y \textit{TBATS} es un modelo lanzado como BATS en 2011 y añade regresores Trigonométricos para modelar múltiples estacionalidades.
Los modelos \textit{BATS} y \textit{TBATS} son métodos de descomposición de una serie temporal que permiten que sus múltiples estacionalidades se incorporen simultáneamente y que cambien lentamente con el tiempo. Cada componente de la serie se estima explícitamente y se mide estadísticamente. Después cada componente estimado se recombina para realizar la predicción final. 
Un par de inconvenientes de los modelos \textit{BATS} y \textit{TBATS} es su lentitud, especialmente con series largas, y que los intervalos de confianza para la predicción suelen ser demasiado amplios.
\smallbreak
Estos dos modelos no admiten regresores. Los hemos aplicado sobre la serie original y sobre la serie transformada logarítmicamente.

	\subsection{Holt-Winters y DSHW}
El modelo de \textit{Holt-Winters} computa el filtrado de \textit{Holt-Winters} de una serie temporal dada, que se fundamenta en la estacionalidad. Este modelo tiene dos variaciones, por una parte la aditiva y por la otra la multiplicativa. Se prefiere la aditiva cuando las variaciones estacionales son en general constantes a lo largo de la serie, mientras que se escogerá la multiplicativa cuando las variaciones estacionales cambien proporcionalmente según el nivel de la serie. Con nuestra serie hemos comprobado que se obtienen mejores resultados cuando la configuración es aditiva. 
\smallbreak
\textit{DSHW (Double Seasonal Holt-Winters)} es un método de 1960 y como su nombre indica es una variación del modelo \textit{Holt-Winters} para series con doble estacionalidad.
Estos modelos no admiten regresores. Los hemos aplicado sobre la serie original y sobre la serie transformada logarítmicamente.

	\subsection{Modelo BSTS} \cite{MODthree} \cite{MODfour} \cite{MODfive} \cite{MODsix} \cite{MODseven}
	El modelo \textit{BSTS} de acuerdo a sus iniciales (Bayesian Structured Time Series) se puede explicar como:
	\begin{itemize}
		\item \underline{Bayesiano}: significa que la implementación tiene un enfoque bayesiano. De manera pragmática, tiene 2 consecuencias principales,
	\begin{enumerate}
		\item  todas las salidas del modelo vendrán en una distribución con un intervalo de certeza (y en realidad todos los parámetros dentro del modelo tendrán una distribución), y 
		\item  es posible expresar el conocimiento previo sobre la serie objetivo a través de los \textit{priors} bayesianos (que pueden considerarse como hiperparámetros del modelo).
	\end{enumerate}
		\item \underline{Estructural}: significa que BSTS proporciona un enfoque estructural para el modelado, en el que hay disponible un kit de componentes para capturar diferentes aspectos de la serie; la arquitectura del modelo puede incluir o excluir cualquiera de esos componentes.
	\item \underline{Time series}: BSTS utiliza modelos de espacio de estado, que es una metodología de modelado en la que el sistema se describe como la combinación de un vector de estado y un vector de observación, ambas series de tiempo. La relación entre el estado y la observación está descrita por el modelo de espacio de estado; el objetivo es inferir las propiedades del estado, que está oculto, de las observaciones disponibles en el pasado. Las previsiones se producen a partir de los estados futuros estimados.
	\end{itemize}

	En sus diferentes formas (modelos de espacio de estado, filtros de Kalman), los modelos BSTS se han utilizado "tradicionalmente" (desde los años 60), y todavía están en uso ya que son muy adecuados para algunos escenarios. No están tan comúnmente cubiertos en los cursos y recursos en línea, tutoriales, etc., y son un poco más difíciles de usar en comparación con otros modelos de aprendizaje automático; sin embargo, hay una biblioteca de código abierto de muy alta calidad disponible en R, razonablemente directa en su uso y bien documentada.
\smallbreak
\subsubsection{Componentes estructurales} 
	BSTS proporciona un kit de componentes que se pueden usar para modelar la serie. Estos componentes capturan diferentes aspectos de la serie, y se pueden agregar o eliminar según las necesidades, como en un juego de construcción. En la descomposición clásica son aproximadamente:
	\[
	y_t  = \underbrace{\mu_t}_{{trend}} + \underbrace{\gamma_t}_{{seasonal}} + \underbrace{\beta^T x_t}_{{regression}} + \epsilon_t
	\]
	\smallbreak
	\[
	\mu_t = \mu_{t-1} + \delta_{t-1} + u_t
	\]
	\smallbreak
	\[
	\delta = \delta_{t-1} + v_t
	\]
	\smallbreak
	\[
	\gamma_t = - \displaystyle\sum_{s=1}^{S-1} \gamma_{t-s} + w_t
	\]
		\begin{itemize}
		\item Componente \textit{trend}, captura los aspectos de tendencia de la serie
		\item Componente \textit{seasonal}, capta los aspectos periódicos de la serie
		\item Componente \textit{regression}, captura la influencia de las variables explicativas (es decir, variables externas a la serie para predecir que proporcionan cierta información a la predicción)
		\end{itemize}
	El kit completo de componentes se encuentra en la descripción de la biblioteca BSTS. \cite{MODeight}

	\subsubsection{Breve descripción del funcionamiento de BSTS}
	BSTS realiza dos operaciones principales: filtrado y suavizado (\textit{filtering and smoothing}). El filtrado proporciona una predicción de la serie dados todos los datos disponibles hasta el momento y el suavizado corrige el estado del modelo cuando una nueva observación de la serie está disponible; es decir, el modelo compara la predicción con la observación y utiliza el error para corregir su propio estado. Al entrenar el modelo la librería revisa internamente todos los datos periodo a periodo, ejecutando sucesivas operaciones de filtrado y suavizado, hasta el último periodo de tiempo disponible.
		\smallbreak
		Es importante tener en cuenta que estas operaciones de filtrado consecutivas se vuelven cada vez menos precisas para un horizonte creciente de la predicción. En nuestra serie evitamos este problema considerando un horizonte de predicción corto, las 12 horas del día siguiente.
		\smallbreak
\underline{Enfoque bayesiano}:
	\smallbreak
	En cada uno de los componentes de un modelo BSTS, es posible configurar \textit{priors} Bayesianos que capturan información previa sobre la serie objetivo para predecir. A primera vista, no parece fácil para un data scientist traducir el conocimiento del negocio sobre la serie a la definición de los {priors}.
		Por ejemplo, el componente \textit{LocalLevel} más simple disponible, que es un componente de tendencia, tiene 2 \textit{priors}:
		\begin{itemize}
		\item \textit{initial.state.prior}: describe la distribución del vector anterior al estado inicial. Esta variable se puede omitir. En la mayoría de las situaciones prácticas, el modelo calcula correctamente el valor inicial del vector de estado sin necesidad de agregar información previa.
		\item \textit{sigma.prior}: se puede interpretar como un parámetro para el suavizado, que determina qué tan ajustado es el componente de tendencia que seguirá los últimos valores de la serie. Un valor alto de \textit{sigma.prior} indica que el componente seguirá la serie con firmeza, y un valor bajo indica lo contrario.
		\end{itemize}
		\smallbreak
		Lo más recomendable es realizar validación cruzada para seleccionar los parámetros con las desventajas que esto conlleva (tiempo excesivo, soluciones buenas pero quizás no las mejores, etc.)

\subsubsection{Modelos de BSTS elegidos}
		Hemos seleccionado tres componentes del modelo BSTS: \textit{LocalLevel}, \textit{Ar} y \textit{LocalLinearTrend}, sabiendo que alguno no es el más adecuado tal y como comenta el diseñador de la librería: \cite{MODnine}
		\smallbreak
		\textit{``El modelo LocalLinearTrend es muy flexible, pero esta flexibilidad puede aparecer como una varianza no deseada en los pronósticos a largo plazo. La mayoría de las variaciones de tus series de tiempo parecen provenir de la estacionalidad, por lo que puedes probar LocalLevel o incluso AddAr en lugar de LocalLinearTrend``}
		\smallbreak
		Además, por cada componente consideramos su aplicación sin regresores y con regresores, considerando las 3 variables externas antes mencionadas.
\smallbreak
\smallbreak
\underline{Componente LocalLinearTrend}:
	\smallbreak
	Agrega un modelo de tendencia lineal local a una especificación de estado. El modelo asume que tanto la media como la pendiente de la tendencia son \textit{random walk}, siendo \textit{random walk} una serie temporal donde la observación actual es igual a la observación previa con una variación aleatoria incremental o decremental, teniendo esa variación una distribución de ruido blanco (sin correlación estadística entre sus valores).
	\smallbreak
	La ecuación para la media es:
	\[
	\mu_{t+1} = \mu_t + \delta_t + rnorm(1,0,sigma.level)
	\]
	\smallbreak
	La pendiente es:
	\[
	\delta_{t+1} = \delta_t + rnorm(1,0,sigma.slope)
	\]
	\smallbreak
	La distribución anterior se encuentra en el nivel de desviación estándar \textit{sigma.level} y la pendiente de desviación estándar \textit{sigma.slope}.
\smallbreak	
\smallbreak
\underline{Componente AR}
	\smallbreak
	Agrega un componente \textit{AR(p)} a una especificación de estado.
	\smallbreak
	El modelo es:
	\[
	\alpha_t = \phi_1 * \alpha_{t-1} + ... + \phi_p * \alpha_{t-p} + \epsilon_{t-1}, con \epsilon_{t-1} \sim N(0, \sigma^2)
	\]
	\smallbreak
	El estado consiste en los últimos valores de $\alpha$. La matriz de transición de estado tiene $\phi$ en su primera fila, valores igual a 1 a lo largo de su primer subdiagonal y valores igual a 0 en el resto. La matriz de varianza del estado tiene $\sigma^2$ en su esquina superior izquierda y es cero en el resto. La matriz de observación tiene el valor 1 en su primer elemento y es cero en el resto.
	\smallbreak
	\smallbreak
\underline{Componente LocalLevel}
\smallbreak
	Agrega un modelo de nivel local a una especificación de estado. El modelo a nivel local asume que la tendencia es un random walk:
	\smallbreak
	\[
	\alpha_{t+1} = \alpha_t + rnorm(1,0,\sigma)
	\]
	\smallbreak
	
%	\begin{figure} [!htb]
%		\centering
%		\includegraphics[scale=.4]{MOD-BSTS-codigoR}
%		\caption{Ejemplo de código R donde se aplica por parquímetro los diferentes componentes}
%		\label{MOD-BSTS-codigoR}
%	\end{figure}

Una de las ventajas de la librería de BSTS es que nos permite obtener gráficas como la siguiente mostrando una comparativa del comportamiento del error acumulado para 3 componentes del modelo con un parquímetro concreto:
	
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-BSTS-grafica}
		\caption{Ejemplo de comparativa de tres componentes BSTS para un parquímetro seleccionado}
		\label{MOD-BSTS-grafica}
	\end{figure}
		
\subsection{Librería SpTimer} \cite{MODeleven} \cite{MODtwelve}

\subsubsection{Modelos espacio-temporales}

SpTimer es una librería para análisis de experimentos que tienen una estructura espacio-temporal. Antes de empezar con spTimer, se describe muy brevemente cómo se modelan eventos espacio-temporales.
De forma sencilla, un modelo espacio-temporal es aquel en el que se han recogido datos de forma espacial (coordenadas) y a intervalos regulares (temporal). Por ejemplo, en nuestro caso de los parquímetros, suponiendo la serie agregada, tendremos datos de ocupación ligados a una coordenada geográfica a intervalos regulares de tiempo. Entonces, la labor del modelo es más complicada que en otros tipos: debe encontrar correlaciones espaciales y correlaciones temporales.

\subsubsection{SpTimer}
SpTimer es una librería que construye modelos espacio-temporales basándose en modelos gaussianos bayesianos. Presenta tres modelos a partir de los cuales se realizan las predicciones:
	\begin{itemize}
		\item Bayesian Gaussian Process (GP)
		\item Bayesian Auto-Regressive Process (AR)
		\item Bayesian Gaussian Predictive Processes (GPP) based AR Model
	\end{itemize}
\smallbreak
Aunque son modelos complicados, es muy importante conocer cuáles son los parámetros que se van a ajustar y cuáles son sus características.
\newpage
\underline{Características y parámetros de los modelos de spTimer}:
\begin{itemize}
		\item Constan de una parte lineal, parecida a una regresión. Estas componentes se suelen llamar \textit{``covariates``} en la literatura, y suelen denotarse por $X_{1t}$. Cada una de esas componentes tiene asociado un parámetro, formándse así un vector de parámetros \textit{regresivos}, $\beta$.
		\item Aparecen dos \textit{componentes de error} diferentes: ``nugget error``, que se asocia con el término de error puro ($\epsilon$), y errores aleatorios espacio-temporales ($\eta$). Matemáticamente, cada una de estas componentes se modela con una varianza ($\sigma^2_{\epsilon}$ y $\sigma^2_{\eta}$).
		\item Hay dos parámetros más: $\phi$,  que controla el ratio con el que la correlación entre dos coordenadas decae conforme aumenta la distancia entre ellas, y $\nu$ que es el orden de la función de Bessel de segunda especie que se utiliza para calcular la correlación espacial.
		\item Son \textit{bayesianos}, en el sentido de que parten de una distribución conocida (distribución a priori o prior) de los parámetros y calculan a partir de ella una distribución final (a posteriori, $\pi$($\cdot \mid z$)).
	\end{itemize}
\smallbreak
En resumen, con cada uno de los tres modelos de spTimer, lo que se trata es de buscar los coeficientes:
	\begin{itemize}
			\item $\beta = (\beta_0, \beta_1, ..., \beta_p)^T$ 
			\item $\sigma^2_{\epsilon}$
			\item $\sigma^2_{\eta}$
			\item $\phi$
			\item $\upsilon$
	\end{itemize}
\smallbreak
La búsqueda se hará para encontrar una aproximación que sea buena. La bondad de la aproximación se mide con el PMCC (Predictive Model Choice Criteria), que es un dato proporcionado por las funciones de la librería.
\smallbreak

\subsubsection{Descripción del dataset utilizado de ejemplo en la librería}
Para estudiar una serie espacio temporal, necesitamos, al menos, varias coordenadas geográficas (2 en este caso: latitud y longitud) y una o varias escalas temporales. Un modelo con 2 escalas temporales, por ejemplo, sería: (año, semana).
Para realizar un estudio previo, la librería utiliza el dataset de taxis de Nueva York (NYdata) que se encuentra dentro de la librería spTimer.
A continuación se cargan y visualizan los datos:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-sptNY}
		\caption{Visualización de los datos de taxis de Nueva York}
		\label{MOD-sptNY}
	\end{figure}
\newpage

Además de las columnas principales (geográficas y temporales), el dataset presenta otras columnas de interés para la predicción. Por orden, son: concentración de ozono, temperatura máxima, velocidad del viento y humedad relativa.
Como conclusión, un dataset para este tipo de problemas debe tener:
	\begin{itemize}
			\item Una o varias columnas geográficas. Normalmente suelen ser dos: latitud y longitud, aunque podrían ser otras. 
			\item Una o varias columnas temporales. Se pueden definir varios niveles dentro de esta variable, para hacer referencia a modelos estacionales:
			\begin{itemize}
				\item Año
				\item Semana
				\item Día
				\item Hora
			\end{itemize}
	\end{itemize}

Estos dos bloques serían los principales. A ellos, se añadirían columnas que representen una característica o propiedad del fenómeno a representar, que ocurra en unas coordenadas geográficas determinadas y en un tiempo determinado. Estas columnas son las que deben agregarse en función de la granularidad que se defina, principalmente, en las columnas temporales.

Finalmente, se hace la predicción sobre una de estas columnas agregadas que hemos añadido.

\subsubsection{Fitting}
\smallbreak
Para realizar el fitting, spTimer muestrea el dataframe con \textit{muestreo de Gibbs} (muestreo utilizado para obtener muestras aleatorias de una distribución conjunta de dos o más variables aleatorias). Después, hace el fitting según los parámetros que le indiquemos. De entre todos los parámetros que hay, los más interesantes para empezar a arrancar el modelo son:
	\begin{itemize}
			\item Cuál es la parte regresiva del modelo (parámetros $\beta$). Se hace a través de un elemento ``fórmula`` de R.
			\item Los datos de origen, con todas las columnas (\textit{data=})
			\item Tipo de modelo: GP, AR o GPP. Es importante destacar que el modelo GPP requiere hacer operaciones adicionales con los datos. GP y AR son más directos. La diferencia entre GP y AR es que, si bien ambos parten de modelos gaussianos, AR incorpora un término de autorregresión ($\rho$) que GP no tiene.
			\item Coordenadas geográficas
			\item Coordenadas temporales
	\end{itemize}

Después, se puede añadir complejidad al modelo con el resto de opciones. Por ejemplo, se pueden definir los priors de los parámetros.
Por ejemplo, con el dataset de NY:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-sptNY2}
		\caption{Ejemplo de código con los datos de taxis de Nueva York}
		\label{MOD-sptNY2}
	\end{figure}
\newpage

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-sptNY3}
		\caption{Resultado de ejecución de código con los datos de taxis de Nueva York}
		\label{MOD-sptNY3}
	\end{figure}

\subsubsection{Parte predictiva}
\smallbreak
En la fase predictiva, sptimer utiliza la función \textit{predict} que requiere dos argumentos:

	\begin{itemize}
			\item \textit{Newcoords}: debe contener los nuevos puntos de coordenadas donde queremos predecir
			\item \textit{Newdata}: que contiene los valores de las covariables
	\end{itemize}

El tipo de argumento en la predicción puede ser ``espacial`` o ``temporal``. Si el valor es ``espacial``, solo se realizará la predicción espacial en el \textit{newcoords} que deben ser diferentes de los sitios ajustados proporcionados por el argumento de \textit{coords}. Cuando se especifica la opción ``temporal``, se realizará el pronóstico y, en este caso, los \textit{newcoords} también pueden contener elementos de los sitios ajustados, en cuyo caso solo se utilizarán datos temporales. Esta función, realizará un pronóstico más allá del último punto de tiempo ajustado.

En cuanto a la medición de la predicción, la librería utiliza la función \textit{spT.validation} que muestra las siguientes métricas:
\newpage
	\begin{itemize}
		\item \textbf{MSE} (mean squared error): $\frac{1}{m}\sum_{i=1}^{m}\left (\widehat{z}-z_{i} \right )^2$
		\item \textbf{RMSE} (root mean squared error): $\sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big(\frac{d_i -f_i}{\sigma_i}\Big)^2}}$
		\item \textbf{MAE} (mean absolute error): $\frac{1}{n}\sum_{t=1}^{n}|e_t|$
		\item \textbf{rBIAS} (relative bias): $\frac{1}{m \overline{z}}\sum_{i=1}^{m}\left (\widehat{z}-z_{i} \right )$
		\item \textbf{rMSEP} (relative mean separation): $\frac{\sum_{i=1}^{m} \left (\widehat{z}-z_{i} \right )^2}{\sum_{i=1}^{m} \left (\overline{z}_p-z_{i} \right )^2}$
	\end{itemize}


\subsubsection{Dataset parking en Seattle para modelos SpTimer}
\smallbreak
En las pruebas que hemos realizado con los modelos spTimer para nuestro dataset, para poder hacer una comparativa hemos desechado la parte espacial del modelo y nos hemos centrado en la parte temporal. El dataset de prueba contiene 30 parquímetros de la ciudad de Seattle con mayor ocupación. 
El modelo para nuestro dataset quedaría de la siguiente manera:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-spt1}
		\caption{Código spTimer para nuestros datos}
		\label{MOD-spt1}
	\end{figure}

Para nuestras pruebas, hemos probado con la combinación de diferentes modelos con los métodos de transformación para la variable de respuesta (\textit{model} y \textit{scale.transform}). De esta manera hemos probado los modelos GP y AR y los métodos de transformación de la variable respuesta SQRT y NONE, este último, el default del modelo. 
Destacar que hemos descartado el modelo GPP en nuestras pruebas. Este modelo trata de definir los efectos aleatorios $\eta (s_{i}, t)$ en un número menor, m, de ubicaciones, llamados \textit{knots} y luego utiliza el método \textit{kriging} para predecir esos efectos aleatorios en los datos y ubicaciones de predicción. 
En la predicción, hemos tomado la parte temporal del modelo para poder así hacer una comparativa con otros.  En esta parte, spTimer incluye el plotting de librería forecast y definiendo el ``site`` que es la posición del \textit{element key} podemos presentar la predicción con gráficos como el siguiente:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-spt2}
		\caption{Representación de la predicción con spTimer}
		\label{MOD-spt2}
	\end{figure}

\section{Modelo descartado: HTS}

HTS es una librería que trata las series temporales relacionadas con estructuras jerárquicas de los datos. Estas estructuras pueden ser dimensiones geográficas, de producto, de género, etc. y son frecuentes en datasets fruto de recopilación de datos.
Estos datos, pueden tener una estructura jerárquica única, o no. Como ejemplo de estructura jerárquica, la documentación de la librería pone de ejemplo datos macroeconómicos donde la renta de un país se desagrega de la siguiente manera: $Y=C+G+I+X-M$, donde \textit{C} es el consumo, \textit{G} el gasto, \textit{I} la inversión, \textit{X} las exportaciones y \textit{M} las importaciones. 
Para una estructura agrupada no jerárquica, el ejemplo indicado es la mortalidad de Australia desagregada por género y a su vez por estados. Pero esta jerarquía no es única, la podemos invertir.
En la literatura estadística existen 3 enfoques en el pronóstico jerárquico de series temporales:

	\begin{itemize}
		\item ``Top-down`` o de arriba hacia abajo.
		\item ``Bottom-up`` o de abajo hacia arriba.
		\item ``Middle-out`` o de enfoque de aproximación de media salida.
	\end{itemize}

Estos métodos, hacen pronósticos en los diferentes niveles y van agregando para obtener un pronóstico mayor. El modelo de hts, implementa estos modelos tomando en cuenta la correlación entre las series en cada nivel.
En el ejemplo práctico de hts, se trata una serie temporal con la mortalidad infantil de Australia de 1933-2003 con 2 etiquetas,  la de género (male or female) y por estados del país tomandose 8, lo que resulta 16 series en el nivel inferior (bottom).
El resultado de la predicción por niveles para los siguientes 10 años estaría representado en estos gráficos:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-hts}
		\caption{Predicciones de ejemplo con la librería HTS}
		\label{MOD-hts}
	\end{figure}

Esta librería nos ha resultado de interés, pero habiendo hecho un estudio de ésta junto con las posibles combinaciones jerárquicas de las variables de nuestro dataset, hemos descartado su prueba e inclusión en la modelización. 

\newpage
\section{Aplicación de los modelos}
Para comparar todos los modelos mencionados con sus distintas variantes (sobre la serie original, o transformada logarítmicamente, o con regresores), utilizamos una técnica llamada \textit{Day Forward-Chaining}. \cite{MODten}

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-valnes}
		\caption{Técnica Day Forward-Chaining}
		\label{MOD-valnes}
	\end{figure}
	
Partimos de nuestra serie temporal acotada al primer trimestre del año con 936 observaciones, correspondientes a 78 días hábiles. 
Comenzamos considerando los 30 primeros días hábiles de la serie (30 días * 12 horas = 360 observaciones) como el primer conjunto de entrenamiento de los modelos, realizamos la primera predicción de las 12 horas del día siguiente, y calculamos el MAE (\textit{Mean Absolute Error}) asociado. 
En el siguiente paso el conjunto de entrenamiento aumenta en 12 observaciones más de la serie y repetimos el proceso con la predicción de las 12 horas del día siguiente y cálculo del MAE asociado. De esta forma el conjunto de entrenamiento crece con cada iteración hasta tener un tamaño final igual a 936 - 12 = 924 observaciones y se repite el proceso por última vez con la predicción de las últimas 12 horas del trimestre y cálculo del MAE asociado. 
Finalmente obtenemos un vector con un total de 48 valores de MAE distintos y calculamos su MAD (\textit{Median Absolute Deviation)} y su Media winsorizada al 5\% superior para descartar posibles outliers. El valor de la Media winsorizada asociada a cada modelo y parquímetro será el parámetro que utilizaremos para comparar los modelos entre sí.      

En el caso de los componentes del modelo BSTS se considera en cada iteración que las últimas 12 observaciones del conjunto de entrenamiento se utilizan para validación y selección de los mejores hiperparámetros del modelo. Y en el caso de spTimer de forma análoga para seleccionar el mejor submodelo entre GP y AR. En la Figura \ref{MOD-bsts-valnes} se muestra el diagrama asociado. 

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{MOD-bsts-valnes}
		\caption{Técnica Day Forward-Chaining con validación (para BSTS y spTimer)}
		\label{MOD-bsts-valnes}
	\end{figure}

Y en la Figura \ref{MOD-vec-maes} representamos para un parquímetro los 48 valores de MAE obtenidos para cada uno los tres mejores modelos, en color negro el mejor modelo, en color azul el segundo y en color verde el tercero y las líneas horizontales en esos tres colores son el valor de la media winsorizada asociada.    

	\begin{center}
		\includegraphics[scale=.4]{MOD-vec-maes}
		\captionof{figure}{MAEs y media winsorizada de tres mejores modelos de un parquímetro seleccionado}
		\label{MOD-vec-maes}
	\end{center}


\chapter{Evaluación de los modelos predictivos}
Por cada uno de los 30 parquímetros seleccionados para el análisis comparativo de los modelos de predicción hemos evaluado un total de 23 submodelos distintos:
\begin{itemize}
\item 4 variaciones del modelo Auto-arima (con y sin regresores; con y sin transformación logarítmica)
\item 2 variaciones del modelo de Medias móviles (con y sin transformación logarítmica)
\item 2 variaciones del modelo MSTL (con y sin transformación logarítmica)
\item 4 variaciones de los modelos BATS y TBATS (cada uno con y sin transformación logarítmica)
\item 4 variaciones de los modelos HW y DSHW (cada uno con y sin transformación logarítmica)
\item 6 variaciones del modelo BSTS (3 componentes distintos, cada uno con y sin regresores)
\item 1 modelo spTimer con elecciones internas del mejor submodelo entre AR o GP
\end{itemize}

Definimos el mejor modelo como aquel que tiene el menor valor de la media winsorizada al 5\% de los MAEs obtenidos en el proceso de validación nesteada comentado en el capítulo anterior.
\smallbreak
En la Figura \ref{RES-top1-all} mostramos cuáles son los mejores modelos y sus resultados asociados para cada uno de los 30 parquímetros seleccionados. Los resultados están ordenados por la columna número de ceros, que cuenta los valores igual a 0 del porcentaje de ocupación, variable objetivo de la predicción y que hemos acotado la serie objetivo del análisis al primer trimestre del año para reducir el coste computacional. Además del predominio de las variaciones de los modelos BATS y TBATS ya mencionadas, vemos también que las transformaciones logarítmicas del modelo BATS y del modelo Holt-Winters son muy útiles para mejorar los resultados en aquellos parquímetros con mayor número de ceros en la variable objetivo de la predicción.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top1-all}
		\caption{Resultados de los mejores modelos para los 30 parquímetros seleccionados}
		\label{RES-top1-all}
	\end{figure}

\newpage
Se observa también que hay gran variabilidad entre los distintos parquímetros para los valores mínimos de la media obtenidos, desde 5.16 hasta 11.245, siendo curioso que los valores extremos mencionados aparezcan para los parquímetros con más ceros (menor media) y menos ceros (mayor media), ya que como se puede comprobar en la Figura \ref{RES-plot-wm-nz} no hay una relación para el resto de los parquímetros, salvo que los valores están más acotados en rango para aquellos parquímetros con mayor número de ceros. Los valores del MAD (desviación absoluta media) están bastante acotados entre 1.26 y 2.88, indicando que hay homogeneidad en los resultados.
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-plot-wm-nz}
		\caption{Relación entre la media mínima de los valores de MAE obtenidos y el número de ceros en la serie}
		\label{RES-plot-wm-nz}
	\end{figure}

En la Figura \ref{RES-top1-models} se muestra la distribución agrupada por familias de modelos. En más de un 76\% de los casos (23 sobre 30 parquímetros) los menores valores de MAE se han obtenido con la familia de modelos TBATS y BATS. Para el resto destacan los modelos HW y DSHW con un 10\% de los casos y los modelos MSTL (6\%, sólo 2 parquímetros). Los modelos Medias móviles y BSTS con componente LocalLevel sin regresores aparecen con 1 parquímetro cada uno.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top1-models}
		\caption{Distribución de los mejores modelos para los 30 parquímetros seleccionados}
		\label{RES-top1-models}
	\end{figure}

\newpage
Por otra parte, comparando los valores de la media obtenida con el mejor modelo frente a la media obtenida con el segundo mejor modelo, vemos que para la mitad de los parquímetros los dos mejores modelos pertenecen a la misma familia, y que para tres parquímetros concretos apenas hay una diferencia de centésimas entre los valores de los dos mejores modelos, aunque en esos tres casos los dos mejores modelos pertenecen a familias distintas. Mostramos los resultados obtenidos para los 3 parquímetros en esa situación:

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top2-ek1045}
		\caption{Resultados de los dos mejores modelos para el parquímetro con id 1045}
		\label{RES-top2-ek1045}
	\end{figure}
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top2-ek62458}
		\caption{Resultados de los dos mejores modelos para el parquímetro con id 62458}
		\label{RES-top2-ek62458}
	\end{figure}
	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top2-ek69098}
		\caption{Resultados de los dos mejores modelos para el parquímetro con id 69098}
		\label{RES-top2-ek69098}
	\end{figure}

\newpage 
Ampliando la evaluación a los 3 mejores modelos por cada parquímetro, obtenemos que se mantiene el predominio de la familia TBATS y BATS pero se reduce al 60\% de los casos, mientras que la familia Holt-Winters aparece destacada en un 22\% de los casos, un 12\% para la familia MSTL y un 5\% para los modelos BSTS.

	\begin{figure} [!htb]
		\centering
		\includegraphics[scale=.4]{RES-top3-models}
		\caption{Distribución de los tres mejores modelos para los 30 parquímetros seleccionados}
		\label{RES-top3-models}
	\end{figure}

Y considerando los 3 mejores modelos encontramos algunas curiosidades:
\begin{itemize}
\item no hay ningún parquímetro que no tenga un modelo de la familia BATS y TBATS entre los 3 mejores
\item hay 3 parquímetros en los que sus 3 mejores modelos son de la misma familia, BATS y TBATS (ids 18622, 30698 y 34938)
\item hay 1 parquímetro donde sus 3 mejores modelos utilizan la serie transformada logarítmicamente (id 13793)
\end{itemize}

Recordamos que para cada parquímetro hemos analizado 23 modelos, que podemos ordenar en un ranking de 0 (mejor modelo) a 22 (peor modelo). Si calculamos la posición media de cada modelo en el listado ordenado de los mejores modelos por parquímetro, obtenemos los resultados que se muestran en la Figura \ref{RES-rank-models-ordered} donde observamos que:
\begin{itemize}
\item el modelo TBATS tiene un mejor rendimiento que BATS
\item DSHW también supera al modelo HW, algo esperable por la doble estacionalidad de nuestra serie
\item los componentes sin regresores de BSTS tienen mejor rendimiento que los que utilizan regresores, y LocalLinearTrend se comporta peor que LocalLevel y Ar
\item la transformación logarítmica no es recomendable para el modelo DSHW que pasa de estar en la cuarta posición a la última
\end{itemize}

	\begin{center}
		\includegraphics[scale=.4]{RES-rank-models-ordered}
		\captionof{figure}{Posición media de cada modelo en el listado de mejores modelos por parquímetro}
		\label{RES-rank-models-ordered}
	\end{center}
 
\chapter{Conclusiones y casos de uso}

\underline{Relevancia de los regresores}
\smallbreak
\underline{Modelos sencillos frente a modelos complejos}
\smallbreak
Un negocio real que podría derivarse de este TFM sería la creación de una app que, utilizando el modelo predictivo TBATS de la librería \textit{forecast} diera una predicción a los usuarios sobre el porcentaje de ocupación de una zona determinada. Por supuesto, somos conscientes que los datos obtenidos para este TFM son datos del pasado, y para poder crear una app que fuera rentable y de utilidad se tendrían que obtener y utilizar datos near-real-time o real-time. 

Cabe señalar que nosotros hemos planteado a lo largo del TFM un estudio basado en la predicción de ocupación por horas del día siguiente. Esto podría simplificar la obtención de los datos, ya que los datos necesarios serían los del día anterior para predecir por horas la ocupación del día siguiente.
Una primera posible mejora para un caso de negocio real sería la franja objetivo de predicción. En vez de agrupar por horas y dar un porcentaje de ocupación horaria, se podría dar una predicción por minutos, o al menos, por medias horas. Esto daría mucha más riqueza y utilidad a los usuarios.

Se podrían obtener los datos de fuentes muy diversas, pero lo menos complicado sería tener un acuerdo con la compañía responsable de los parquímetros, para asegurar la calidad de los datos. En Seattle lo han conseguido desde la app \textit{PayByPhone} \cite{CDUone}, que es una app desde la cual poder hacer los pagos de los parquímetros. Hasta tal punto ha llegado la satisfacción con esta app, que el Departamento de Transportes de Seattle (SDOT) ha ampliado su acuerdo con la misma, eliminando el sobrecoste de 35 céntimos que antes incluía a los usuarios de esta app en el pago de sus tickets. \cite{CDUtwo}
Otra manera que permitiría la obtención de datos podría ser la instalación de sensores, lo cual daría al negocio cierta independencia del proveedor de los datos, aunque la contrapartida inevitable sería el alto coste de los mismos. Con los sensores se podrían tener datos muy actualizados, pero el coste de partida de las infraestructuras necesarias para el funcionamiento de la app sería bastante elevado.
También se podría aprovechar la información de los mismos usuarios, que compartirían con la app su geolocalización y la proveerían de datos. Éstos compartirían su localización y actualizarían el estado de las plazas libres. Sin embargo, ésta no podría ser la única fuente de datos, pues sin usuarios la app no lograría progresar.

Sería conveniente, antes incluso de negociar con la empresa de parquímetros, llevar a cabo un estudio de mercado analizando la demanda estimada que pudiera tener este servicio. Lo lógico sería centrarse en núcleos urbanos superpoblados donde el aparcamiento en la calle suponga una odisea para sus ciudadanos, y donde reine el aparcamiento regulado mediante parquímetros. Además habría que fijarse si estamos ante un nicho de mercado o, si por el contrario, ya existen competidores ofreciendo lo mismo o muy parecido. Por una parte, es positivo llegar los primeros a un mercado porque somos los pioneros y abrimos un servicio, creando una demanda que antes ni se planteaba; además de que disfrutaremos de momentos de tranquilidad solos ante los consumidores, aprovechando esa ventaja para fidelizarlos, por ejemplo. Por otra parte, es innegable que si somos los pioneros asumiremos un riesgo, el riesgo de que no se conozca nuestro producto y tengamos que esforzarnos mucho en darlo a conocer y hacer ver a los ciudadanos las ventajas de su uso; otros competidores siempre aportan riqueza a un negocio, procurando mejoras continuas y agilizando la oferta de los servicios, por no mencionar que si hay ya competidores significa que existe el mercado y la necesidad ya está creada en los consumidores.

\printbibliography	

\end{document}